{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchfly\n",
    "torchfly.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import random\n",
    "import itertools\n",
    "import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    \n",
    "from apex import amp\n",
    "from allennlp.training.checkpointer import Checkpointer\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torchfly.text.tokenizers import UnifiedBPETokenizer\n",
    "\n",
    "from torchfly.modules.losses import SequenceFocalLoss, SequenceCrossEntropyLoss\n",
    "from torchfly.modules.transformers import GPT2SimpleLM, UnifiedGPT2SmallConfig\n",
    "from text_utils import recoverText, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tokenizer\n",
    "tokenizer = UnifiedBPETokenizer()\n",
    "tokenizer.sep_token = \"None\"\n",
    "# add speicial tokens in the same order as Roberta\n",
    "# tokenizer.add_tokens([\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "class GPT2SmallConfig:\n",
    "    vocab_size = 50257 + len(tokenizer.added_tokens_encoder)\n",
    "    n_special = len(tokenizer.added_tokens_encoder)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 768\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = False\n",
    "    \n",
    "class GPT2MediumConfig:\n",
    "    vocab_size = len(tokenizer.added_tokens_encoder)\n",
    "    n_special = len(tokenizer.added_tokens_encoder)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 1024\n",
    "    n_layer = 24\n",
    "    n_head = 16\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = True\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A = GPT2SimpleLM(UnifiedGPT2SmallConfig)\n",
    "model_B = GPT2SimpleLM(UnifiedGPT2SmallConfig)\n",
    "model_A.load_state_dict(torch.load(\"../../Checkpoint/best.th\"))\n",
    "model_B.load_state_dict(torch.load(\"../../Checkpoint/best.th\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_keep_indices(batch_keep_indices):\n",
    "    prev = batch_keep_indices[1]\n",
    "    new_batch_keep_indices = [prev]\n",
    "\n",
    "    for i in range(1, len(batch_keep_indices)):\n",
    "        curr = batch_keep_indices[i]\n",
    "        new = [50140, 50118]\n",
    "\n",
    "        for idx in curr:\n",
    "            new.append(prev.index(idx))\n",
    "\n",
    "        new_batch_keep_indices.append(new)\n",
    "        prev = curr\n",
    "        \n",
    "    return new_batch_keep_indices\n",
    "\n",
    "\n",
    "class MultiWOZDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bos = tokenizer.encode(\"<s>\")\n",
    "        self.user_bos = tokenizer.encode(\"A:\")\n",
    "        self.system_bos = tokenizer.encode(\"B:\")\n",
    "        \n",
    "        self.eos = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        full_dialog = self.data[index]['log']\n",
    "        \n",
    "        full_dialog_tokens = []\n",
    "        cur_pos = 0\n",
    "        \n",
    "        for turn_dialog in full_dialog:\n",
    "            cur_pos = 0\n",
    "            \n",
    "            # user\n",
    "            user = recoverText(turn_dialog['user_delex'])\n",
    "            # user = recoverText(turn_dialog['user_delex'])\n",
    "            user_tokens = self.user_bos + tokenizer.encode(user) + self.eos\n",
    "\n",
    "            user_pos = torch.arange(cur_pos, cur_pos + len(user_tokens))\n",
    "            #cur_pos = user_pos[-1] + 1\n",
    "            \n",
    "            # belief span\n",
    "#             belief_tokens = self.bos + \\\n",
    "#                             tokenizer.encode(\";\".join(turn_dialog['bspan_inform'][1:])) + \\\n",
    "#                             self.eos\n",
    "#             belief_pos = torch.arange(cur_pos, cur_pos + len(belief_tokens))\n",
    "#             cur_pos = belief_pos[-1]\n",
    "\n",
    "\n",
    "            # Database\n",
    "            if eval(turn_dialog['pointer'])[-2:] == (1, 0):\n",
    "                booked = \"book\"\n",
    "            elif eval(turn_dialog['pointer'])[-2:] == (0, 1):\n",
    "                booked = \"fail\"\n",
    "            else:\n",
    "                booked = \"none\"\n",
    "            \n",
    "            if len(turn_dialog['match']) > 0:\n",
    "                num_match = int(turn_dialog['match']) if int(turn_dialog['match']) < 4 else 4\n",
    "            else:\n",
    "                num_match = 0\n",
    "            \n",
    "            database = str(num_match) + \";\" + booked + \";\" + turn_dialog['turn_domain'].strip(\"[]\") + \";\"\n",
    "            database_tokens = tokenizer.encode(database)\n",
    "            database_pos = torch.arange(cur_pos, cur_pos + len(database_tokens))\n",
    "            cur_pos = database_pos[-1] + 1\n",
    "            \n",
    "            # System\n",
    "            system = recoverText(turn_dialog['resp'])\n",
    "            system_tokens = self.system_bos + tokenizer.encode(system) + self.eos\n",
    "            system_pos = torch.arange(cur_pos, cur_pos + len(system_tokens))\n",
    "            # cur_pos = system_pos[-1] + 1\n",
    "            \n",
    "            user_tokens = torch.LongTensor(user_tokens)\n",
    "            system_tokens = torch.LongTensor(system_tokens)\n",
    "            database_tokens = torch.LongTensor(database_tokens)\n",
    "            \n",
    "            full_dialog_tokens.append((user_tokens, \n",
    "                                       user_pos,\n",
    "                                       system_tokens, \n",
    "                                       system_pos,\n",
    "                                       database_tokens,\n",
    "                                       database_pos))\n",
    "#             if system_pos[-1] > 1:\n",
    "#                 break\n",
    "\n",
    "        return full_dialog_tokens\n",
    "        \n",
    "\n",
    "class Collate_Function:\n",
    "    \"\"\"This function handles batch collate.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad = self.tokenizer.encode(\"<pad>\")[0]\n",
    "\n",
    "    def __call__(self, unpacked_data):\n",
    "        \n",
    "        import pickle\n",
    "        with open(\"unpacked_data\", \"wb\") as f:\n",
    "            pickle.dump(unpacked_data, f)\n",
    "            \n",
    "        exit(0)\n",
    "\n",
    "        max_turn_len = max([len(item) for item in unpacked_data])\n",
    "        \n",
    "        batch_dialogs = []\n",
    "        batch_keep_indices = []\n",
    "            \n",
    "        for turn_num in range(max_turn_len):\n",
    "\n",
    "            keep_indices = []\n",
    "\n",
    "            for batch_idx in range(len(unpacked_data)):\n",
    "                if turn_num < len(unpacked_data[batch_idx]):\n",
    "                    \n",
    "                    # breakpoint()\n",
    "                    keep_indices.append(batch_idx)\n",
    "\n",
    "            user_tokens = pad_sequence([unpacked_data[idx][turn_num][0] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            user_pos = pad_sequence([unpacked_data[idx][turn_num][1] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)\n",
    "            system_tokens = pad_sequence([unpacked_data[idx][turn_num][2] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            system_pos = pad_sequence([unpacked_data[idx][turn_num][3] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)\n",
    "            database_tokens = pad_sequence([unpacked_data[idx][turn_num][4] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            database_pos = pad_sequence([unpacked_data[idx][turn_num][5] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)  \n",
    "\n",
    "            user_mask = (user_tokens != self.pad).byte()\n",
    "            system_mask = (system_tokens != self.pad).byte()\n",
    "            database_mask = (database_tokens != self.pad).byte()\n",
    "\n",
    "\n",
    "            batch_dialogs.append((user_tokens, user_pos, user_mask, \n",
    "                                  system_tokens, system_pos, system_mask, \n",
    "                                  database_tokens, database_pos, database_mask))\n",
    "            batch_keep_indices.append(keep_indices)\n",
    "            \n",
    "        # align keep indices\n",
    "        # batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "        \n",
    "        return batch_dialogs, batch_keep_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(logits, target, mask):\n",
    "    logits = logits[:, :-1].contiguous()\n",
    "    target = target[:, 1:].contiguous()\n",
    "    mask = mask[:, 1:].contiguous().float()\n",
    "    loss = criterion(logits, target, mask, label_smoothing=0.02, reduce=True)\n",
    "    return loss\n",
    "\n",
    "def filter_past(past, keep_indices):\n",
    "    past = [item[:, keep_indices] for item in past]\n",
    "    return past\n",
    "\n",
    "def replace_punc(x):\n",
    "    x = x.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "    return x.replace(\".\", \" .\").replace(\",\", \" .\").replace(\"?\", \" ?\").replace(\"?\", \" ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO should be clean_train_data.json\n",
    "with open(\"yichi_data/train_data.json\") as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "with open(\"yichi_data/val_data.json\") as f:\n",
    "    val_data = json.load(f)\n",
    "    \n",
    "with open(\"yichi_data/test_data.json\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "indices = np.arange(len(train_data))\n",
    "np.random.shuffle(indices)\n",
    "# use all data\n",
    "indices = indices\n",
    "train_data = [train_data[idx] for idx in indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiWOZDataset(train_data, tokenizer)\n",
    "val_dataset = MultiWOZDataset(val_data, tokenizer)\n",
    "test_dataset = MultiWOZDataset(test_data, tokenizer)\n",
    "\n",
    "train_batch_size = 2\n",
    "collate_func = Collate_Function(tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              shuffle=True,\n",
    "                              batch_size=train_batch_size, \n",
    "                              collate_fn=collate_func)\n",
    "\n",
    "eval_batch_size = 4\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                          shuffle=False,\n",
    "                          batch_size=eval_batch_size, \n",
    "                          collate_fn=collate_func)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                          shuffle=False,\n",
    "                          batch_size=eval_batch_size, \n",
    "                          collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SequenceFocalLoss(gamma=0.0, beta=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_A = model_A.to(device)\n",
    "model_B = model_B.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = Checkpointer(serialization_dir=\"Checkpoint\", \n",
    "                            keep_serialized_model_every_num_seconds=3600*2, \n",
    "                            num_serialized_models_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "num_epochs = 10\n",
    "num_gradients_accumulation = 1\n",
    "num_train_optimization_steps = num_train_optimization_steps = len(train_dataset) * num_epochs // train_batch_size // num_gradients_accumulation\n",
    "\n",
    "param_optimizer = list(model_A.named_parameters()) + list(model_B.named_parameters())\n",
    "no_decay = ['ln', 'bias', 'LayerNorm']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=3e-5,\n",
    "                  correct_bias=False)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                 num_warmup_steps=100,\n",
    "                                 num_training_steps=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [model_A, model_B], optimizer = amp.initialize([model_A, model_B], optimizer, opt_level=\"O1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_weight = 1.0\n",
    "\n",
    "def train_one_iter(batch_dialogs, batch_keep_indices, update_count, fp16=False):\n",
    "\n",
    "    aligned_batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "   \n",
    "    mask = torch.ByteTensor([]).to(device)\n",
    "    prev_batch_size = batch_dialogs[0][0].shape[0]\n",
    "    \n",
    "    past = None\n",
    "    all_logits = []\n",
    "    target = []\n",
    "    total_loss = 0 \n",
    "    \n",
    "    for turn_num in range(len(batch_keep_indices)):\n",
    "\n",
    "        # data send to gpu\n",
    "        dialogs = batch_dialogs[turn_num]\n",
    "        dialogs = [item.to(device) for item in dialogs]\n",
    "\n",
    "        user_tokens, user_pos, user_mask, \\\n",
    "            system_tokens, system_pos, system_mask, \\\n",
    "            database_tokens, database_pos, database_mask = dialogs\n",
    "        \n",
    "        # breakpoint()\n",
    "\n",
    "        # filtering algorithm\n",
    "        keep_indices = aligned_batch_keep_indices[turn_num]\n",
    "\n",
    "        if len(keep_indices) != prev_batch_size:\n",
    "            past = filter_past(past, keep_indices)\n",
    "            mask = mask[keep_indices, :]\n",
    "\n",
    "        # User Utterance\n",
    "        mask = torch.cat([mask, user_mask], dim=-1)\n",
    "        logits, past = model_A(user_tokens, position_ids=user_pos, mask=mask, past=past)\n",
    "        A_loss = calculate_loss(logits, user_tokens, user_mask)\n",
    "\n",
    "        # Database Tokens\n",
    "        mask = torch.cat([mask, database_mask], dim=-1)\n",
    "        logits, past = model_B(database_tokens, position_ids=database_pos, mask=mask, past=past)\n",
    "        database_loss = calculate_loss(logits, database_tokens, database_mask)        \n",
    "        \n",
    "        # System Response\n",
    "        mask = torch.cat([mask, system_mask], dim=-1)\n",
    "        logits, past = model_B(system_tokens, position_ids=system_pos, mask=mask, past=past)\n",
    "        B_loss = calculate_loss(logits, system_tokens, system_mask)\n",
    "\n",
    "        # tail\n",
    "        total_loss = total_loss + user_weight * A_loss + B_loss + database_loss\n",
    "        prev_batch_size = user_tokens.shape[0]\n",
    "\n",
    "#     breakpoint\n",
    "#     all_logits = torch.cat(all_logits, dim=1)\n",
    "#     all_logits = all_logits[:, :-1].contiguous()\n",
    "\n",
    "#     target = torch.cat(target, dim=1)\n",
    "#     target = target[:, 1:].contiguous()\n",
    "    \n",
    "#     target_mask = torch.ones_like(target).float()\n",
    "    \n",
    "#     total_loss = criterion(all_logits, target, target_mask, label_smoothing=0.02, reduce=True)\n",
    "\n",
    "    # gradient accumulation\n",
    "    total_loss /= len(batch_keep_indices)\n",
    "    total_loss /= num_gradients_accumulation \n",
    "    \n",
    "    if fp16:\n",
    "        with amp.scale_loss(total_loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "    else:\n",
    "        total_loss.backward()\n",
    "        \n",
    "    record_loss = total_loss.item() * num_gradients_accumulation\n",
    "    perplexity = np.exp(record_loss)\n",
    "    \n",
    "    return record_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinggu/anaconda3/envs/pretraining_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d576ee583f4cacb5282bc3635ef8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4210), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinggu/.local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.92 GiB total capacity; 9.98 GiB already allocated; 5.50 MiB free; 317.98 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4272c269a6f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m205\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mrecord_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dialogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_keep_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m# breakpoint()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mupdate_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ca71e803931e>\u001b[0m in \u001b[0;36mtrain_one_iter\u001b[0;34m(batch_dialogs, batch_keep_indices, update_count, fp16)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mrecord_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_gradients_accumulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.92 GiB total capacity; 9.98 GiB already allocated; 5.50 MiB free; 317.98 MiB cached)"
     ]
    }
   ],
   "source": [
    "update_count = 0\n",
    "progress_bar = tqdm.tqdm_notebook\n",
    "start = time.time()\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "\n",
    "    \"Training\"\n",
    "    pbar = progress_bar(train_dataloader)\n",
    "    model_A.train()\n",
    "    model_B.train()\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for batch_dialogs, batch_keep_indices in pbar:\n",
    "        \n",
    "        i += 1\n",
    "        if i<205:\n",
    "            continue\n",
    "        record_loss, perplexity = train_one_iter(batch_dialogs, batch_keep_indices, update_count, fp16=False)\n",
    "        # breakpoint()\n",
    "        update_count += 1\n",
    "\n",
    "        if update_count % num_gradients_accumulation == num_gradients_accumulation - 1:\n",
    "            # update for gradient accumulation\n",
    "            scheduler.step()\n",
    "            torch.nn.utils.clip_grad_norm_(model_A.parameters(), 5.0)\n",
    "            torch.nn.utils.clip_grad_norm_(model_B.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # speed measure\n",
    "            end = time.time()\n",
    "            speed = train_batch_size * num_gradients_accumulation / (end - start)\n",
    "            start = end\n",
    "            \n",
    "            # show progress\n",
    "            pbar.set_postfix(loss=record_loss, perplexity=perplexity, speed=speed)\n",
    "    \n",
    "#   \"Evaluation\"\n",
    "    print(f\"Epoch {ep} Validation\")\n",
    "    eval_res = validate(val_dataloader, val_data)\n",
    "    print(eval_res)\n",
    "    print(f\"Epoch {ep} Test\")\n",
    "    eval_res = validate(test_dataloader, test_data)\n",
    "    print(eval_res)\n",
    "    \n",
    "    checkpointer.save_checkpoint(ep, \n",
    "                                 [model_A.state_dict(), model_B.state_dict()],\n",
    "                                 {\"None\": None},\n",
    "                                 True\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"unpacked_data\", \"rb\") as f:\n",
    "    unpacked_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_turn_len = max([len(item) for item in unpacked_data])\n",
    "\n",
    "batch_dialogs = []\n",
    "batch_keep_indices = []\n",
    "\n",
    "for turn_num in range(max_turn_len):\n",
    "\n",
    "    keep_indices = []\n",
    "\n",
    "    for batch_idx in range(len(unpacked_data)):\n",
    "        if turn_num < len(unpacked_data[batch_idx]):\n",
    "\n",
    "            # breakpoint()\n",
    "            keep_indices.append(batch_idx)\n",
    "    print(keep_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pretraining_env]",
   "language": "python",
   "name": "conda-env-pretraining_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
