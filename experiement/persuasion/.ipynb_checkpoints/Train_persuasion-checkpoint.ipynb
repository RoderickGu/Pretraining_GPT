{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import random\n",
    "import itertools\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    \n",
    "from apex import amp\n",
    "from allennlp.training.checkpointer import Checkpointer\n",
    "from gpt_model import GPT2SimpleLM\n",
    "# TODO why openaiadam?\n",
    "#from pytorch_pretrained_bert import GPT2Tokenizer, OpenAIAdam, GPT2Model\n",
    "from pytorch_pretrained_bert import OpenAIAdam\n",
    "from transformers import AdamW\n",
    "from transformers import WarmupLinearSchedule\n",
    "from torchfly.text.tokenizers import UnifiedBPETokenizer\n",
    "from torchfly.modules.losses import SequenceFocalLoss, SequenceCrossEntropyLoss\n",
    "from torchfly.modules.transformers import GPT2SimpleLM, UnifiedGPT2SmallConfig\n",
    "\n",
    "# TODO no warmup learning rate schedule?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersuadeDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.max_len = 1500\n",
    "        self.turn_ending = tokenizer.encode(\"\\n\\n\\n\")\n",
    "        # TODO: no ending?\n",
    "        # self.dialog_ending = [tokenizer.encoder[\"[EOS]\"]]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        dial_tokens = [tokenizer.encode(item) + self.turn_ending for item in self.data[index]]\n",
    "        role_ids = [0 if item[0] == 32 else 1 for item in dial_tokens]\n",
    "        dial_tokens[-1] = dial_tokens[-1][:-2] # + self.dialog_ending\n",
    "        return role_ids, dial_tokens\n",
    "        \n",
    "\n",
    "class Collate_Function:\n",
    "    \"\"\"This function handles batch collate.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        # self.EOS = self.tokenizer.encoder[\"[EOS]\"]\n",
    "\n",
    "    def __call__(self, unpacked_data):\n",
    "        return unpacked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass GPT2SmallConfig:\\n    vocab_size = 50257 + len(tokenizer.__special_tokens__)\\n    n_special = len(tokenizer.__special_tokens__)\\n    n_positions = 1024\\n    n_ctx = 1024\\n    n_embd = 768\\n    n_layer = 12\\n    n_head = 12\\n    resid_pdrop = 0.1\\n    embd_pdrop = 0.1\\n    attn_pdrop = 0.1\\n    layer_norm_epsilon = 1e-5\\n    initializer_range = 0.02\\n    gradient_checkpointing = False\\n    \\nclass GPT2MediumConfig:\\n    vocab_size = 50257 + len(tokenizer.__special_tokens__)\\n    n_special = len(tokenizer.__special_tokens__)\\n    n_positions = 1024\\n    n_ctx = 1024\\n    n_embd = 1024\\n    n_layer = 24\\n    n_head = 16\\n    resid_pdrop = 0.1\\n    embd_pdrop = 0.1\\n    attn_pdrop = 0.1\\n    layer_norm_epsilon = 1e-5\\n    initializer_range = 0.02\\n    gradient_checkpointing = True\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = UnifiedBPETokenizer()\n",
    "\n",
    "#tokenizer = torch.load(\"/home/qingyang/Desktop/GPT2_Modification/special3_gpt2_tokenizer.pkl\")\n",
    "# TODO why load tokneizer\n",
    "'''\n",
    "class GPT2SmallConfig:\n",
    "    vocab_size = 50257 + len(tokenizer.__special_tokens__)\n",
    "    n_special = len(tokenizer.__special_tokens__)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 768\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = False\n",
    "    \n",
    "class GPT2MediumConfig:\n",
    "    vocab_size = 50257 + len(tokenizer.__special_tokens__)\n",
    "    n_special = len(tokenizer.__special_tokens__)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 1024\n",
    "    n_layer = 24\n",
    "    n_head = 16\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = True\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A = GPT2SimpleLM(UnifiedGPT2SmallConfig)\n",
    "model_B = GPT2SimpleLM(UnifiedGPT2SmallConfig)\n",
    "#model_A.load_state_dict(torch.load(\"/home/qingyang/Desktop/GPT2_Modification/special3_gpt2_small.pth\"))\n",
    "#model_B.load_state_dict(torch.load(\"/home/qingyang/Desktop/GPT2_Modification/special3_gpt2_small.pth\"))\n",
    "model_A.load_state_dict(torch.load(\"../../Checkpoint/best.th\"))\n",
    "model_B.load_state_dict(torch.load(\"../../Checkpoint/best.th\"))\n",
    "\n",
    "# model_A = GPT2SimpleLM(GPT2MediumConfig)\n",
    "# model_B = GPT2SimpleLM(GPT2MediumConfig)\n",
    "# model_A.load_state_dict(torch.load(\"/home/qingyang/Desktop/GPT2_Modification/special3_gpt2_medium.pth\"))\n",
    "# model_B.load_state_dict(torch.load(\"/home/qingyang/Desktop/GPT2_Modification/special3_gpt2_medium.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load(\"DataProcess/train_dialogs.pkl\")\n",
    "val_data = torch.load(\"DataProcess/val_dialogs.pkl\")\n",
    "\n",
    "train_dataset = PersuadeDataset(train_data, tokenizer)\n",
    "val_dataset = PersuadeDataset(val_data, tokenizer)\n",
    "\n",
    "batch_size = 1\n",
    "collate_func = Collate_Function(tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              shuffle=True, \n",
    "                              batch_size=batch_size, \n",
    "                              collate_fn=collate_func)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                            shuffle=False, \n",
    "                            batch_size=batch_size, \n",
    "                            collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model_A = model_A.to(device)\n",
    "model_B = model_B.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the losses\n",
    "criterion = SequenceFocalLoss(gamma=1.0, beta=0.0)\n",
    "eval_criterion = SequenceCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_iter(batch, update_count, fp16=False):\n",
    "    role_ids, dialog_tokens = batch\n",
    "    dial_inputs = [torch.LongTensor(item).unsqueeze(0).to(device) for item in dialog_tokens]\n",
    "    \n",
    "    past = None\n",
    "    all_logits = []\n",
    "    # A_logits = []\n",
    "    # B_logits = []\n",
    "    # A_target = []\n",
    "    # B_target = []\n",
    "#     user = tokenizer.encode(\"B:\" + user)\n",
    "#     sep = tokenizer.encode(\"\\n\\n\\n\") \n",
    "#     suffix = tokenizer.encode(\"A:\")\n",
    "#     prev_input = sep + user + sep + suffix\n",
    "    \n",
    "#     prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(device)\n",
    "#     past_length = past_position_ids.item()\n",
    "    \n",
    "#     past_position_ids = np.arange(past_length, past_length+2).tolist() + \\\n",
    "#                          np.arange(len(user) + 2).tolist() + \\\n",
    "#                          np.arange(2).tolist()\n",
    "    \n",
    "#     past_position_ids = torch.LongTensor(past_position_ids).unsqueeze(0).to(device)\n",
    "    \n",
    "    for turn_num, dial_turn_inputs in enumerate(dial_inputs):\n",
    "        if role_ids[turn_num] == 0:\n",
    "            # breakpoint()\n",
    "            logits, past = model_A(dial_turn_inputs, past=past)\n",
    "            all_logits.append(logits)\n",
    "        else:\n",
    "            # breakpoint()\n",
    "            logits, past = model_B(dial_turn_inputs, past=past)\n",
    "            all_logits.append(logits)\n",
    "\n",
    "    all_logits = torch.cat(all_logits, dim=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # target\n",
    "    all_logits = all_logits[:, :-1].contiguous()\n",
    "    target = torch.cat(dial_inputs, dim=1)[:, 1:].contiguous()\n",
    "    target_mask = torch.ones_like(target).float()\n",
    "    \n",
    "    loss = criterion(all_logits, target, target_mask, label_smoothing=0.02, reduce=True)\n",
    "    loss /= num_gradients_accumulation\n",
    "    \n",
    "    if fp16:\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "    else:\n",
    "        loss.backward()\n",
    "        \n",
    "    record_loss = loss.item() * num_gradients_accumulation\n",
    "    perplexity = np.exp(record_loss)\n",
    "    \n",
    "    return record_loss, perplexity\n",
    "\n",
    "\n",
    "def validate(dataloader):\n",
    "    with torch.no_grad():\n",
    "        pbar = progress_bar(dataloader)\n",
    "\n",
    "        total_ppl = []\n",
    "\n",
    "        for batch in pbar:\n",
    "            \n",
    "            if sum([len(item) for item in batch[0][1]]) > 1024:\n",
    "                continue\n",
    "            \n",
    "            role_ids, dialog_tokens = batch[0]\n",
    "            dial_inputs = [torch.LongTensor(item).unsqueeze(0).to(device) for item in dialog_tokens]\n",
    "\n",
    "            past = None\n",
    "            all_logits = []\n",
    "            # A_logits = []\n",
    "            # B_logits = []\n",
    "            # A_target = []\n",
    "            # B_target = []\n",
    "\n",
    "            for turn_num, dial_turn_inputs in enumerate(dial_inputs):\n",
    "                if role_ids[turn_num] == 0:\n",
    "                    logits, past = model_A(dial_turn_inputs, past=past)\n",
    "                    all_logits.append(logits)\n",
    "                else:\n",
    "                    logits, past = model_B(dial_turn_inputs, past=past)\n",
    "                    all_logits.append(logits)\n",
    "\n",
    "            all_logits = torch.cat(all_logits, dim=1)\n",
    "            \n",
    "            # target\n",
    "            all_logits = all_logits[:, :-1].contiguous()\n",
    "            target = torch.cat(dial_inputs, dim=1)[:, 1:].contiguous()\n",
    "            target_mask = torch.ones_like(target).float()\n",
    "            \n",
    "            loss = eval_criterion(all_logits, target, target_mask, label_smoothing=-1, reduce=\"sentence\")      \n",
    "\n",
    "            ppl = torch.exp(loss)\n",
    "            total_ppl.extend(ppl.tolist())\n",
    "\n",
    "        print(f\"Epcoh {ep} Validation Perplexity: {np.mean(total_ppl)} Variance: {np.var(total_ppl)}\")\n",
    "        \n",
    "        return np.mean(total_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"Checkpoint\"):\n",
    "    os.makedirs(\"Checkpoint\")\n",
    "checkpointer = Checkpointer(serialization_dir=\"Checkpoint\", \n",
    "                            keep_serialized_model_every_num_seconds=3600*2, \n",
    "                            num_serialized_models_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "num_epochs = 10\n",
    "num_gradients_accumulation = 1\n",
    "num_train_optimization_steps = num_train_optimization_steps = len(train_dataset) * num_epochs // batch_size // num_gradients_accumulation\n",
    "\n",
    "param_optimizer = list(model_A.named_parameters()) + list(model_B.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "\n",
    "optimizer = OpenAIAdam(optimizer_grouped_parameters,\n",
    "                       lr=2e-5,\n",
    "                       warmup=0.1,\n",
    "                       max_grad_norm=1.0,\n",
    "                       weight_decay=0.01,\n",
    "                       t_total=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support fp16\n",
    "# [model_A, model_B], optimizer = amp.initialize([model_A, model_B], optimizer, opt_level=\"O1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinggu/anaconda3/envs/pretraining_env/lib/python3.7/site-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382446938bcb4b91bfc5dd4d9376db69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=891), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinggu/anaconda3/envs/pretraining_env/lib/python3.7/site-packages/ipykernel_launcher.py:61: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af8c4ceed96496c836176e131eea34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epcoh 0 Validation Perplexity: 11.629383223397392 Variance: 12.327919333045331\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Checkpoint/model_state_epoch_0.th'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-43467b9d7964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mis_best_so_far\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppl\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mold_ppl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mold_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mcheckpointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_B\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_best_so_far\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pretraining_env/lib/python3.7/site-packages/allennlp/training/checkpointer.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, epoch, model_state, training_states, is_best_so_far)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serialization_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_serialization_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_state_epoch_{}.th\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             training_path = os.path.join(self._serialization_dir,\n\u001b[1;32m     44\u001b[0m                                          \"training_state_epoch_{}.th\".format(epoch))\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \"\"\"\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Checkpoint/model_state_epoch_0.th'"
     ]
    }
   ],
   "source": [
    "update_count = 0\n",
    "progress_bar = tqdm.tqdm_notebook\n",
    "start = time.time()\n",
    "old_ppl = -float('Inf')\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "\n",
    "    \"Training\"\n",
    "    pbar = progress_bar(train_dataloader)\n",
    "    model_A.train()\n",
    "    model_B.train()\n",
    "    \n",
    "    for batch in pbar:\n",
    "        batch = batch[0]\n",
    "        # without relative position\n",
    "        if sum([len(item) for item in batch[1]]) > 1024:\n",
    "            continue\n",
    "            \n",
    "        record_loss, perplexity = train_one_iter(batch, update_count, fp16=False)\n",
    "        \n",
    "        update_count += 1\n",
    "\n",
    "        if update_count % num_gradients_accumulation == num_gradients_accumulation - 1:\n",
    "            # update for gradient accumulation\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # speed measure\n",
    "            end = time.time()\n",
    "            speed = batch_size * num_gradients_accumulation / (end - start)\n",
    "            start = end\n",
    "            \n",
    "            # show progress\n",
    "            pbar.set_postfix(loss=record_loss, perplexity=perplexity, speed=speed)\n",
    "\n",
    "    \"Evaluation\"\n",
    "    model_A.eval()\n",
    "    model_B.eval()\n",
    "    ppl = validate(val_dataloader)\n",
    "    \n",
    "    is_best_so_far = ppl > old_ppl\n",
    "    old_ppl = ppl\n",
    "    checkpointer.save_checkpoint(ep, [model_A.state_dict(), model_B.state_dict()], {\"None\": None}, is_best_so_far)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pretraining_env]",
   "language": "python",
   "name": "conda-env-pretraining_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
