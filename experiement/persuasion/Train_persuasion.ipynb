{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import random\n",
    "import itertools\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    \n",
    "from apex import amp\n",
    "from allennlp.training.checkpointer import Checkpointer\n",
    "from gpt_model import GPT2SimpleLM\n",
    "# TODO why openaiadam?\n",
    "#from pytorch_pretrained_bert import GPT2Tokenizer, OpenAIAdam, GPT2Model\n",
    "from pytorch_pretrained_bert import OpenAIAdam\n",
    "from transformers import AdamW\n",
    "from transformers import WarmupLinearSchedule\n",
    "from torchfly.text.tokenizers import UnifiedBPETokenizer\n",
    "from torchfly.modules.losses import SequenceFocalLoss, SequenceCrossEntropyLoss\n",
    "from torchfly.modules.transformers import GPT2SimpleLM, UnifiedGPT2SmallConfig\n",
    "\n",
    "# TODO no warmup learning rate schedule?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersuadeDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.max_len = 1500\n",
    "        self.turn_ending = tokenizer.encode(\"\\n\\n\\n\")\n",
    "        # TODO: no ending?\n",
    "        # self.dialog_ending = [tokenizer.encoder[\"[EOS]\"]]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        dial_tokens = [tokenizer.encode(item) + self.turn_ending for item in self.data[index]]\n",
    "        role_ids = [0 if item[0] == 32 else 1 for item in dial_tokens]\n",
    "        dial_tokens[-1] = dial_tokens[-1][:-2] # + self.dialog_ending\n",
    "        return role_ids, dial_tokens\n",
    "        \n",
    "\n",
    "class Collate_Function:\n",
    "    \"\"\"This function handles batch collate.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        # self.EOS = self.tokenizer.encoder[\"[EOS]\"]\n",
    "\n",
    "    def __call__(self, unpacked_data):\n",
    "        return unpacked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass GPT2SmallConfig:\\n    vocab_size = 50257 + len(tokenizer.__special_tokens__)\\n    n_special = len(tokenizer.__special_tokens__)\\n    n_positions = 1024\\n    n_ctx = 1024\\n    n_embd = 768\\n    n_layer = 12\\n    n_head = 12\\n    resid_pdrop = 0.1\\n    embd_pdrop = 0.1\\n    attn_pdrop = 0.1\\n    layer_norm_epsilon = 1e-5\\n    initializer_range = 0.02\\n    gradient_checkpointing = False\\n    \\nclass GPT2MediumConfig:\\n    vocab_size = 50257 + len(tokenizer.__special_tokens__)\\n    n_special = len(tokenizer.__special_tokens__)\\n    n_positions = 1024\\n    n_ctx = 1024\\n    n_embd = 1024\\n    n_layer = 24\\n    n_head = 16\\n    resid_pdrop = 0.1\\n    embd_pdrop = 0.1\\n    attn_pdrop = 0.1\\n    layer_norm_epsilon = 1e-5\\n    initializer_range = 0.02\\n    gradient_checkpointing = True\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = UnifiedBPETokenizer()\n",
    "\n",
    "#tokenizer = torch.load(\"/home/qingyang/Desktop/GPT2_Modification/special3_gpt2_tokenizer.pkl\")\n",
    "# TODO why load tokneizer\n",
    "'''\n",
    "class GPT2SmallConfig:\n",
    "    vocab_size = 50257 + len(tokenizer.__special_tokens__)\n",
    "    n_special = len(tokenizer.__special_tokens__)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 768\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = False\n",
    "    \n",
    "class GPT2MediumConfig:\n",
    "    vocab_size = 50257 + len(tokenizer.__special_tokens__)\n",
    "    n_special = len(tokenizer.__special_tokens__)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 1024\n",
    "    n_layer = 24\n",
    "    n_head = 16\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = True\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A = GPT2SimpleLM(UnifiedGPT2SmallConfig)\n",
    "model_B = GPT2SimpleLM(UnifiedGPT2SmallConfig)\n",
    "#model_A.load_state_dict(torch.load(\"/home/qingyang/Desktop/GPT2_Modification/special3_gpt2_small.pth\"))\n",
    "#model_B.load_state_dict(torch.load(\"/home/qingyang/Desktop/GPT2_Modification/special3_gpt2_small.pth\"))\n",
    "model_A.load_state_dict(torch.load(\"../../Checkpoint/best.th\"))\n",
    "model_B.load_state_dict(torch.load(\"../../Checkpoint/best.th\"))\n",
    "\n",
    "# model_A = GPT2SimpleLM(GPT2MediumConfig)\n",
    "# model_B = GPT2SimpleLM(GPT2MediumConfig)\n",
    "# model_A.load_state_dict(torch.load(\"/home/qingyang/Desktop/GPT2_Modification/special3_gpt2_medium.pth\"))\n",
    "# model_B.load_state_dict(torch.load(\"/home/qingyang/Desktop/GPT2_Modification/special3_gpt2_medium.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load(\"DataProcess/train_dialogs.pkl\")\n",
    "val_data = torch.load(\"DataProcess/val_dialogs.pkl\")\n",
    "\n",
    "train_dataset = PersuadeDataset(train_data, tokenizer)\n",
    "val_dataset = PersuadeDataset(val_data, tokenizer)\n",
    "\n",
    "batch_size = 1\n",
    "collate_func = Collate_Function(tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              shuffle=True, \n",
    "                              batch_size=batch_size, \n",
    "                              collate_fn=collate_func)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                            shuffle=False, \n",
    "                            batch_size=batch_size, \n",
    "                            collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model_A = model_A.to(device)\n",
    "model_B = model_B.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the losses\n",
    "criterion = SequenceFocalLoss(gamma=1.0, beta=0.0)\n",
    "eval_criterion = SequenceCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_iter(batch, update_count, fp16=False):\n",
    "    role_ids, dialog_tokens = batch\n",
    "    dial_inputs = [torch.LongTensor(item).unsqueeze(0).to(device) for item in dialog_tokens]\n",
    "    \n",
    "    past = None\n",
    "    all_logits = []\n",
    "    # A_logits = []\n",
    "    # B_logits = []\n",
    "    # A_target = []\n",
    "    # B_target = []\n",
    "#     user = tokenizer.encode(\"B:\" + user)\n",
    "#     sep = tokenizer.encode(\"\\n\\n\\n\") \n",
    "#     suffix = tokenizer.encode(\"A:\")\n",
    "#     prev_input = sep + user + sep + suffix\n",
    "    \n",
    "#     prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(device)\n",
    "#     past_length = past_position_ids.item()\n",
    "    \n",
    "#     past_position_ids = np.arange(past_length, past_length+2).tolist() + \\\n",
    "#                          np.arange(len(user) + 2).tolist() + \\\n",
    "#                          np.arange(2).tolist()\n",
    "    \n",
    "#     past_position_ids = torch.LongTensor(past_position_ids).unsqueeze(0).to(device)\n",
    "    \n",
    "    for turn_num, dial_turn_inputs in enumerate(dial_inputs):\n",
    "        if role_ids[turn_num] == 0:\n",
    "            # breakpoint()\n",
    "            logits, past = model_A(dial_turn_inputs, past=past)\n",
    "            all_logits.append(logits)\n",
    "        else:\n",
    "            # breakpoint()\n",
    "            logits, past = model_B(dial_turn_inputs, past=past)\n",
    "            all_logits.append(logits)\n",
    "\n",
    "    all_logits = torch.cat(all_logits, dim=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # target\n",
    "    all_logits = all_logits[:, :-1].contiguous()\n",
    "    target = torch.cat(dial_inputs, dim=1)[:, 1:].contiguous()\n",
    "    target_mask = torch.ones_like(target).float()\n",
    "    \n",
    "    loss = criterion(all_logits, target, target_mask, label_smoothing=0.02, reduce=True)\n",
    "    loss /= num_gradients_accumulation\n",
    "    \n",
    "    if fp16:\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "    else:\n",
    "        loss.backward()\n",
    "        \n",
    "    record_loss = loss.item() * num_gradients_accumulation\n",
    "    perplexity = np.exp(record_loss)\n",
    "    \n",
    "    return record_loss, perplexity\n",
    "\n",
    "\n",
    "def validate(dataloader):\n",
    "    with torch.no_grad():\n",
    "        pbar = progress_bar(dataloader)\n",
    "\n",
    "        total_ppl = []\n",
    "\n",
    "        for batch in pbar:\n",
    "            \n",
    "            if sum([len(item) for item in batch[0][1]]) > 1024:\n",
    "                continue\n",
    "            \n",
    "            role_ids, dialog_tokens = batch[0]\n",
    "            dial_inputs = [torch.LongTensor(item).unsqueeze(0).to(device) for item in dialog_tokens]\n",
    "\n",
    "            past = None\n",
    "            all_logits = []\n",
    "            # A_logits = []\n",
    "            # B_logits = []\n",
    "            # A_target = []\n",
    "            # B_target = []\n",
    "\n",
    "            for turn_num, dial_turn_inputs in enumerate(dial_inputs):\n",
    "                if role_ids[turn_num] == 0:\n",
    "                    logits, past = model_A(dial_turn_inputs, past=past)\n",
    "                    all_logits.append(logits)\n",
    "                else:\n",
    "                    logits, past = model_B(dial_turn_inputs, past=past)\n",
    "                    all_logits.append(logits)\n",
    "\n",
    "            all_logits = torch.cat(all_logits, dim=1)\n",
    "            \n",
    "            # target\n",
    "            all_logits = all_logits[:, :-1].contiguous()\n",
    "            target = torch.cat(dial_inputs, dim=1)[:, 1:].contiguous()\n",
    "            target_mask = torch.ones_like(target).float()\n",
    "            \n",
    "            loss = eval_criterion(all_logits, target, target_mask, label_smoothing=-1, reduce=\"sentence\")      \n",
    "\n",
    "            ppl = torch.exp(loss)\n",
    "            total_ppl.extend(ppl.tolist())\n",
    "\n",
    "        print(f\"Epcoh {ep} Validation Perplexity: {np.mean(total_ppl)} Variance: {np.var(total_ppl)}\")\n",
    "        \n",
    "        return np.mean(total_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"Checkpoint\"):\n",
    "    os.makedirs(\"Checkpoint\")\n",
    "checkpointer = Checkpointer(serialization_dir=\"Checkpoint\", \n",
    "                            keep_serialized_model_every_num_seconds=3600*2, \n",
    "                            num_serialized_models_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "num_epochs = 10\n",
    "num_gradients_accumulation = 1\n",
    "num_train_optimization_steps = num_train_optimization_steps = len(train_dataset) * num_epochs // batch_size // num_gradients_accumulation\n",
    "\n",
    "param_optimizer = list(model_A.named_parameters()) + list(model_B.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "\n",
    "optimizer = OpenAIAdam(optimizer_grouped_parameters,\n",
    "                       lr=2e-5,\n",
    "                       warmup=0.1,\n",
    "                       max_grad_norm=1.0,\n",
    "                       weight_decay=0.01,\n",
    "                       t_total=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support fp16\n",
    "# [model_A, model_B], optimizer = amp.initialize([model_A, model_B], optimizer, opt_level=\"O1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinggu/anaconda3/envs/pretraining_env/lib/python3.7/site-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e39841f0df4f7b9300803d61f22704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=891), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinggu/anaconda3/envs/pretraining_env/lib/python3.7/site-packages/ipykernel_launcher.py:61: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70eef206dd77463fb70b73755cff3595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epcoh 0 Validation Perplexity: 11.629383223397392 Variance: 12.327919333045331\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d044f8da999448cc80156493093c61d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=891), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3b3d972a504be2a92b5124f671b9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epcoh 1 Validation Perplexity: 10.62478511187495 Variance: 11.51269794930117\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acdf7d5d14e4e5db796328651f59fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=891), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a379a538a9a14dedafeb5fc8d9d760c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epcoh 2 Validation Perplexity: 10.249737620353699 Variance: 11.211154902251376\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d59aed44604363947ba5775d640914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=891), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82f62acdd3a47f1b3c341018259bced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epcoh 3 Validation Perplexity: 10.079965934461477 Variance: 11.183145393877847\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f304d3d36f1045a689eacaf28bb6d96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=891), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b2c23e325146b081775d604959c10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epcoh 4 Validation Perplexity: 10.000443351512052 Variance: 11.250616878880448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c0a298190142e196ef314ec2d96822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=891), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421d1b917dec4716b9da7d8e963e9891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epcoh 5 Validation Perplexity: 9.968954276065437 Variance: 11.509323373219022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f2d85ba35249dc8a56cc2c54ac566c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=891), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b440935333c043f78b05063bc07633c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epcoh 6 Validation Perplexity: 9.929912992886134 Variance: 11.60251898158781\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c108d3496094f769e8e0cadce6e3830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=891), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18639f0447343c991e93a5a70046f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epcoh 7 Validation Perplexity: 9.92309160865083 Variance: 11.715699135269219\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f60512a8dbf4cfea6c58fcc02c244ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=891), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9473f2a6b0943f095bc14ec43602884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epcoh 8 Validation Perplexity: 9.932806625658152 Variance: 11.829686047241344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e6f328b1a84f33a503cd4732c734d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=891), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d3d841d11d495f8586d31ebe817788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=99), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epcoh 9 Validation Perplexity: 9.939598545736196 Variance: 11.870388044289164\n"
     ]
    }
   ],
   "source": [
    "update_count = 0\n",
    "progress_bar = tqdm.notebook.tqdm\n",
    "start = time.time()\n",
    "old_ppl = -float('Inf')\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "\n",
    "    \"Training\"\n",
    "    pbar = progress_bar(train_dataloader)\n",
    "    model_A.train()\n",
    "    model_B.train()\n",
    "    \n",
    "    for batch in pbar:\n",
    "        batch = batch[0]\n",
    "        # without relative position\n",
    "        if sum([len(item) for item in batch[1]]) > 1024:\n",
    "            continue\n",
    "            \n",
    "        record_loss, perplexity = train_one_iter(batch, update_count, fp16=False)\n",
    "        \n",
    "        update_count += 1\n",
    "\n",
    "        if update_count % num_gradients_accumulation == num_gradients_accumulation - 1:\n",
    "            # update for gradient accumulation\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # speed measure\n",
    "            end = time.time()\n",
    "            speed = batch_size * num_gradients_accumulation / (end - start)\n",
    "            start = end\n",
    "            \n",
    "            # show progress\n",
    "            pbar.set_postfix(loss=record_loss, perplexity=perplexity, speed=speed)\n",
    "\n",
    "    \"Evaluation\"\n",
    "    model_A.eval()\n",
    "    model_B.eval()\n",
    "    ppl = validate(val_dataloader)\n",
    "    \n",
    "    is_best_so_far = ppl > old_ppl\n",
    "    old_ppl = ppl\n",
    "    checkpointer.save_checkpoint(ep, [model_A.state_dict(), model_B.state_dict()], {\"None\": None}, is_best_so_far)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pretraining_env]",
   "language": "python",
   "name": "conda-env-pretraining_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
