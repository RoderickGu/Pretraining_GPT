{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cloudpickle as pickle\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_pretrained_bert import GPT2Model, GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "from gpt_model import GPT2SimpleLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the 117M gpt2\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the 117M gpt2\n",
    "model_medium = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "speical_tokens = [\n",
    "    \"[PAD]\",\n",
    "    \"[SEP]\",\n",
    "    \"[EOS]\",\n",
    "]\n",
    "\n",
    "# add them to encoder\n",
    "for i in range(len(speical_tokens)):\n",
    "    tokenizer.encoder[speical_tokens[i]] = 50257 + i\n",
    "\n",
    "# add them to decoder\n",
    "for i in range(len(speical_tokens)):\n",
    "    tokenizer.decoder[50257 + i] = speical_tokens[i]\n",
    "\n",
    "setattr(tokenizer, \"__special_tokens__\", speical_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21886]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tokenizer, \"special3_gpt2_tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_interpolate(x):\n",
    "    return x[np.random.randint(50257, size=20), :].mean(0) + \\\n",
    "            torch.randn(x.shape[-1]) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_special_tokens = len(speical_tokens)\n",
    "\n",
    "# copy the original embedding\n",
    "new_embedding = nn.Embedding(model.config.vocab_size + num_special_tokens, model.config.n_embd)\n",
    "new_embedding.weight.data[:model.config.vocab_size, :] = model.transformer.wte.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the first three, use random interpolate\n",
    "for i in range(3):\n",
    "    new_embedding.weight.data[model.config.vocab_size+i, :] = random_interpolate(new_embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.wte = new_embedding\n",
    "model.lm_head.decoder.weight = model.transformer.wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2SmallConfig:\n",
    "    vocab_size = 50257 + len(speical_tokens)\n",
    "    n_special = len(speical_tokens)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 768\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_states = model.state_dict()\n",
    "model_states = {k: v for k, v in model_states.items() if '.attn.bias' not in k}\n",
    "\n",
    "new_model_small = GPT2SimpleLM(GPT2SmallConfig)\n",
    "new_model_small.load_state_dict(model_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_special_tokens = len(speical_tokens)\n",
    "\n",
    "# copy the original embedding\n",
    "new_embedding = nn.Embedding(model_medium.config.vocab_size + num_special_tokens, model_medium.config.n_embd)\n",
    "new_embedding.weight.data[:model_medium.config.vocab_size, :] = model_medium.transformer.wte.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the first three, use random interpolate\n",
    "for i in range(3):\n",
    "    new_embedding.weight.data[model_medium.config.vocab_size+i, :] = random_interpolate(new_embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_medium.transformer.wte = new_embedding\n",
    "model_medium.lm_head.decoder.weight = model_medium.transformer.wte.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2MediumConfig:\n",
    "    vocab_size = 50257 + len(speical_tokens)\n",
    "    n_special = len(speical_tokens)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 1024\n",
    "    n_layer = 24\n",
    "    n_head = 16\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_medium_states = model_medium.state_dict()\n",
    "model_medium_states = {k: v for k, v in model_medium_states.items() if '.attn.bias' not in k}\n",
    "\n",
    "new_model_medium = GPT2SimpleLM(GPT2MediumConfig)\n",
    "new_model_medium.load_state_dict(model_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
