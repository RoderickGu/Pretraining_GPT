{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchfly\n",
    "torchfly.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import random\n",
    "import itertools\n",
    "import tqdm\n",
    "import time\n",
    "from torchfly.utils.model_utils import get_pretrained_states\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except:\n",
    "    from tensorboardX import SummaryWriter\n",
    "from apex import amp\n",
    "from allennlp.training.checkpointer import Checkpointer\n",
    "# from pytorch_transformers import AdamW, WarmupLinearSchedule, GPT2Tokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torchfly.text.tokenizers import UnifiedBPETokenizer\n",
    "\n",
    "from torchfly.modules.losses import SequenceFocalLoss, SequenceCrossEntropyLoss\n",
    "from torchfly.modules.transformers import GPT2SimpleLM, UnifiedGPT2SmallConfig\n",
    "from cam676_eval.cam676_eval import clean_sentence, entities, entity_dict, success_f1_metric, bleu_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tokenizer\n",
    "tokenizer = UnifiedBPETokenizer()\n",
    "tokenizer.sep_token = \"None\"\n",
    "# add speicial tokens in the same order as Roberta\n",
    "# tokenizer.add_tokens([\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass GPT2SmallConfig:\\n    vocab_size = 50257 + len(tokenizer.added_tokens_encoder)\\n    n_special = len(tokenizer.added_tokens_encoder)\\n    n_positions = 1024\\n    n_ctx = 1024\\n    n_embd = 768\\n    n_layer = 12\\n    n_head = 12\\n    resid_pdrop = 0.1\\n    embd_pdrop = 0.1\\n    attn_pdrop = 0.1\\n    layer_norm_epsilon = 1e-5\\n    initializer_range = 0.02\\n    gradient_checkpointing = False\\n    \\nclass GPT2MediumConfig:\\n    vocab_size = len(tokenizer.added_tokens_encoder)\\n    n_special = len(tokenizer.added_tokens_encoder)\\n    n_positions = 1024\\n    n_ctx = 1024\\n    n_embd = 1024\\n    n_layer = 24\\n    n_head = 16\\n    resid_pdrop = 0.1\\n    embd_pdrop = 0.1\\n    attn_pdrop = 0.1\\n    layer_norm_epsilon = 1e-5\\n    initializer_range = 0.02\\n    gradient_checkpointing = True\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class GPT2SmallConfig:\n",
    "    vocab_size = 50257 + len(tokenizer.added_tokens_encoder)\n",
    "    n_special = len(tokenizer.added_tokens_encoder)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 768\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = False\n",
    "    \n",
    "class GPT2MediumConfig:\n",
    "    vocab_size = len(tokenizer.added_tokens_encoder)\n",
    "    n_special = len(tokenizer.added_tokens_encoder)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 1024\n",
    "    n_layer = 24\n",
    "    n_head = 16\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = True\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = GPT2SimpleLM(UnifiedGPT2SmallConfig)\n",
    "model_B = GPT2SimpleLM(UnifiedGPT2SmallConfig)\n",
    "# model_A.load_state_dict(torch.load(\"../../../Checkpoint/best.th\"))\n",
    "# model_B.load_state_dict(torch.load(\"../../../Checkpoint/best.th\"))\n",
    "# model_A.load_state_dict(get_pretrained_states(\"unified-gpt2-small\"))\n",
    "# model_B.load_state_dict(get_pretrained_states(\"unified-gpt2-small\"))\n",
    "model_A.load_state_dict(torch.load(\"/data/jinggu/project/Pretraining_GPT/mc_version_torchfly_gpt_small\"))\n",
    "model_B.load_state_dict(torch.load(\"/data/jinggu/project/Pretraining_GPT/mc_version_torchfly_gpt_small\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_keep_indices(batch_keep_indices):\n",
    "    prev = batch_keep_indices[1]\n",
    "    new_batch_keep_indices = [prev]\n",
    "\n",
    "    for i in range(1, len(batch_keep_indices)):\n",
    "        curr = batch_keep_indices[i]\n",
    "        new = []\n",
    "\n",
    "        for idx in curr:\n",
    "            new.append(prev.index(idx))\n",
    "\n",
    "        new_batch_keep_indices.append(new)\n",
    "        prev = curr\n",
    "        \n",
    "    return new_batch_keep_indices\n",
    "\n",
    "\n",
    "class CamRestDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bos = tokenizer.encode(\"<s>\")\n",
    "        self.user_bos = tokenizer.encode(\"A:\")\n",
    "        self.system_bos = tokenizer.encode(\"B:\")\n",
    "        \n",
    "        self.eos = [628, 198]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        full_dialog = self.data[index]\n",
    "        \n",
    "        full_dialog_tokens = []\n",
    "        cur_pos = 0\n",
    "        \n",
    "        for turn_dialog in full_dialog:\n",
    "            # user\n",
    "            user_tokens = self.user_bos + tokenizer.encode(turn_dialog['user']) + self.eos\n",
    "            user_pos = torch.arange(cur_pos, cur_pos + len(user_tokens))\n",
    "            cur_pos = user_pos[-1] + 1\n",
    "            \n",
    "            # belief span\n",
    "            belief_tokens = self.bos + \\\n",
    "                            tokenizer.encode(\";\".join(turn_dialog['bspan_inform'][1:])) + \\\n",
    "                            self.eos\n",
    "            belief_pos = torch.arange(cur_pos, cur_pos + len(belief_tokens))\n",
    "            cur_pos = belief_pos[-1]\n",
    "            \n",
    "            # system\n",
    "            if np.random.rand() < 0.04:\n",
    "                turn_dialog[\"degree\"] = 0 \n",
    "            database = tokenizer.encode(str(turn_dialog[\"degree\"]))\n",
    "            # database_pos = torch.LongTensor([1023])\n",
    "            \n",
    "            system_tokens = self.system_bos + \\\n",
    "                            tokenizer.encode(turn_dialog['replaced_response']) + \\\n",
    "                            self.eos\n",
    "            system_pos = torch.arange(cur_pos, cur_pos + len(system_tokens) + 1)\n",
    "            cur_pos = system_pos[-1] + 1\n",
    "            \n",
    "            # concat database and response\n",
    "            system_tokens = database + system_tokens\n",
    "            # system_pos = torch.cat([database_pos, system_pos], dim=0)\n",
    "            \n",
    "            user_tokens = torch.LongTensor(user_tokens)\n",
    "            system_tokens = torch.LongTensor(system_tokens)\n",
    "            belief_tokens = torch.LongTensor(belief_tokens)\n",
    "            \n",
    "            full_dialog_tokens.append((user_tokens, \n",
    "                                       user_pos, \n",
    "                                       system_tokens, \n",
    "                                       system_pos, \n",
    "                                       belief_tokens, \n",
    "                                       belief_pos))\n",
    "\n",
    "        return full_dialog_tokens\n",
    "        \n",
    "\n",
    "class Collate_Function:\n",
    "    \"\"\"This function handles batch collate.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad = self.tokenizer.encode(\"<pad>\")[0]\n",
    "\n",
    "    def __call__(self, unpacked_data):\n",
    "\n",
    "        max_turn_len = max([len(item) for item in unpacked_data])\n",
    "        \n",
    "        batch_dialogs = []\n",
    "        batch_keep_indices = []\n",
    "\n",
    "        for turn_num in range(max_turn_len):\n",
    "\n",
    "            keep_indices = []\n",
    "\n",
    "            for batch_idx in range(len(unpacked_data)):\n",
    "                if turn_num < len(unpacked_data[batch_idx]):\n",
    "                    keep_indices.append(batch_idx)\n",
    "\n",
    "            user_tokens = pad_sequence([unpacked_data[idx][turn_num][0] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            user_pos = pad_sequence([unpacked_data[idx][turn_num][1] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)\n",
    "            system_tokens = pad_sequence([unpacked_data[idx][turn_num][2] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            system_pos = pad_sequence([unpacked_data[idx][turn_num][3] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)\n",
    "            belief_tokens = pad_sequence([unpacked_data[idx][turn_num][4] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            belief_pos = pad_sequence([unpacked_data[idx][turn_num][5] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)  \n",
    "\n",
    "            user_mask = (user_tokens != self.pad).byte()\n",
    "            system_mask = (system_tokens != self.pad).byte()\n",
    "            belief_mask = (belief_tokens != self.pad).byte()\n",
    "\n",
    "\n",
    "            batch_dialogs.append((user_tokens, user_pos, user_mask, \n",
    "                                  system_tokens, system_pos, system_mask, \n",
    "                                  belief_tokens, belief_pos, belief_mask))\n",
    "            batch_keep_indices.append(keep_indices)\n",
    "            \n",
    "        # align keep indices\n",
    "        # batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "        return batch_dialogs, batch_keep_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(logits, target, mask):\n",
    "    logits = logits[:, :-1].contiguous()\n",
    "    target = target[:, 1:].contiguous()\n",
    "    mask = mask[:, 1:].contiguous().float()\n",
    "    loss = criterion(logits, target, mask, label_smoothing=0.02, reduce=True)\n",
    "    return loss\n",
    "\n",
    "def filter_past(past, keep_indices):\n",
    "    past = [item[:, keep_indices] for item in past]\n",
    "    return past\n",
    "\n",
    "def replace_punc(x):\n",
    "    x = x.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "    return x.replace(\".\", \" .\").replace(\",\", \" .\").replace(\"?\", \" ?\").replace(\"?\", \" ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load(\"../data/DataProcess/train_data.pkl\")\n",
    "val_data = torch.load(\"../data/DataProcess/val_data.pkl\")\n",
    "test_data = torch.load(\"../data/DataProcess/test_data.pkl\")\n",
    "\n",
    "indices = np.arange(len(train_data))\n",
    "np.random.shuffle(indices)\n",
    "# use all data\n",
    "indices = indices[: 200]\n",
    "train_data = [train_data[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CamRestDataset(train_data, tokenizer)\n",
    "val_dataset = CamRestDataset(val_data, tokenizer)\n",
    "test_dataset = CamRestDataset(test_data, tokenizer)\n",
    "\n",
    "train_batch_size = 1\n",
    "collate_func = Collate_Function(tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              shuffle=True,\n",
    "                              batch_size=train_batch_size, \n",
    "                              collate_fn=collate_func)\n",
    "\n",
    "eval_batch_size = 16\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                          shuffle=False,\n",
    "                          batch_size=eval_batch_size, \n",
    "                          collate_fn=collate_func)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                          shuffle=False,\n",
    "                          batch_size=eval_batch_size, \n",
    "                          collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SequenceFocalLoss(gamma=0.0, beta=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_A = model_A.to(device)\n",
    "model_B = model_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"Checkpoint\"):\n",
    "    os.makedirs(\"Checkpoint\")\n",
    "checkpointer = Checkpointer(serialization_dir=\"Checkpoint\", \n",
    "                            keep_serialized_model_every_num_seconds=3600*2, \n",
    "                            num_serialized_models_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "num_epochs = 10\n",
    "num_gradients_accumulation = 1\n",
    "num_train_optimization_steps = num_train_optimization_steps = len(train_dataset) * num_epochs // train_batch_size // num_gradients_accumulation\n",
    "\n",
    "param_optimizer = list(model_A.named_parameters()) + list(model_B.named_parameters())\n",
    "no_decay = ['ln', 'bias', 'LayerNorm']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=5e-5,\n",
    "                  correct_bias=False)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                 num_warmup_steps=500,\n",
    "                                 num_training_steps=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [model_A, model_B], optimizer = amp.initialize([model_A, model_B], optimizer, opt_level=\"O0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_weight = 1.0\n",
    "\n",
    "def train_one_iter(batch_dialogs, batch_keep_indices, update_count, fp16=False):\n",
    "\n",
    "    aligned_batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "   \n",
    "    mask = torch.ByteTensor([]).to(device)\n",
    "    prev_batch_size = batch_dialogs[0][0].shape[0]\n",
    "    \n",
    "\n",
    "    past = None\n",
    "    all_logits = []\n",
    "    target = []\n",
    "    total_loss = 0 \n",
    "    \n",
    "    for turn_num in range(len(batch_keep_indices)):\n",
    "\n",
    "        # data send to gpu\n",
    "        dialogs = batch_dialogs[turn_num]\n",
    "        dialogs = [item.to(device) for item in dialogs]\n",
    "\n",
    "        user_tokens, user_pos, user_mask, \\\n",
    "            system_tokens, system_pos, system_mask, \\\n",
    "            belief_tokens, belief_pos, belief_mask = dialogs\n",
    "\n",
    "        # filtering algorithm\n",
    "        keep_indices = aligned_batch_keep_indices[turn_num]\n",
    "\n",
    "        if len(keep_indices) != prev_batch_size:\n",
    "            past = filter_past(past, keep_indices)\n",
    "            mask = mask[keep_indices, :]\n",
    "\n",
    "        # User Utterance\n",
    "        mask = torch.cat([mask, user_mask], dim=-1)\n",
    "        logits, past = model_A(user_tokens, position_ids=user_pos, mask=mask, past=past)\n",
    "        all_logits.append(logits)\n",
    "        target.append(user_tokens)\n",
    "        # A_loss = calculate_loss(logits, user_tokens, user_mask)\n",
    "\n",
    "        # System Response\n",
    "        mask = torch.cat([mask, system_mask], dim=-1)\n",
    "        logits, past = model_B(system_tokens, position_ids=system_pos, mask=mask, past=past)\n",
    "        all_logits.append(logits)\n",
    "        target.append(system_tokens)\n",
    "        # B_loss = calculate_loss(logits, system_tokens, system_mask)\n",
    "\n",
    "        # tail\n",
    "        # total_loss = total_loss + user_weight * A_loss + B_loss\n",
    "        prev_batch_size = user_tokens.shape[0]\n",
    "\n",
    "#     breakpoint\n",
    "    all_logits = torch.cat(all_logits, dim=1)\n",
    "    all_logits = all_logits[:, :-1].contiguous()\n",
    "\n",
    "    target = torch.cat(target, dim=1)\n",
    "    target = target[:, 1:].contiguous()\n",
    "    \n",
    "    target_mask = torch.ones_like(target).float()\n",
    "    \n",
    "    total_loss = criterion(all_logits, target, target_mask, label_smoothing=0.02, reduce=True)\n",
    "\n",
    "    # gradient accumulation\n",
    "    total_loss /= len(batch_keep_indices)\n",
    "    total_loss /= num_gradients_accumulation \n",
    "    \n",
    "    if fp16:\n",
    "        with amp.scale_loss(total_loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "    else:\n",
    "        total_loss.backward()\n",
    "        \n",
    "    record_loss = total_loss.item() * num_gradients_accumulation\n",
    "    perplexity = np.exp(record_loss)\n",
    "    \n",
    "    return record_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, data):\n",
    "\n",
    "    model_A.eval()\n",
    "    model_B.eval()\n",
    "\n",
    "    temperature = 0.5\n",
    "\n",
    "    all_response = []\n",
    "\n",
    "    for batch_dialogs, batch_keep_indices in tqdm.notebook.tqdm(dataloader):\n",
    "\n",
    "        aligned_batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "        past = None\n",
    "        generated_responses = [[] for i in range(batch_dialogs[0][0].shape[0])]\n",
    "\n",
    "        mask = torch.ByteTensor([]).to(device)\n",
    "        prev_batch_size = batch_dialogs[0][0].shape[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for turn_num in range(len(batch_keep_indices)):\n",
    "                # data send to gpu\n",
    "                dialogs = batch_dialogs[turn_num]\n",
    "                dialogs = [item.to(device) for item in dialogs]\n",
    "\n",
    "                user_tokens, user_pos, user_mask, \\\n",
    "                    system_tokens, system_pos, system_mask, \\\n",
    "                    belief_tokens, belief_pos, belief_mask = dialogs\n",
    "\n",
    "                # batch filtering algorithm\n",
    "                keep_indices = aligned_batch_keep_indices[turn_num]\n",
    "\n",
    "                if len(keep_indices) != prev_batch_size:\n",
    "                    past = filter_past(past, keep_indices)\n",
    "                    mask = mask[keep_indices, :]\n",
    "\n",
    "                # define some initials\n",
    "                cur_batch_size = user_tokens.shape[0]\n",
    "                flags = np.ones(cur_batch_size)\n",
    "                generated_tokens = [[] for i in range(cur_batch_size)]\n",
    "\n",
    "                # feed in user\n",
    "                mask = torch.cat([mask, user_mask], dim=-1)\n",
    "                _, past = model_A(user_tokens, position_ids=user_pos, mask=mask, past=past)\n",
    "\n",
    "                # response generation\n",
    "                response = []\n",
    "\n",
    "\n",
    "                # first three tokens\n",
    "                prev_input = system_tokens[:, :3]\n",
    "                cur_pos = system_pos[:, :3]\n",
    "                temp_past = past\n",
    "                temp_mask = F.pad(mask, pad=(0,3), value=1)\n",
    "\n",
    "                # feed into B\n",
    "                logits, temp_past = model_B(prev_input, position_ids=cur_pos, mask=temp_mask, past=temp_past)\n",
    "                # set current position\n",
    "                cur_pos = cur_pos[:, -1].unsqueeze(1) + 1\n",
    "\n",
    "                for i in range(50):\n",
    "                    logits = logits[:, -1, :] / temperature\n",
    "                    prev_tokens = torch.argmax(logits, dim=-1)\n",
    "                    np_prev_tokens = prev_tokens.cpu().numpy()\n",
    "                    # nucleus sampling\n",
    "                    # logits = top_filtering(logits, top_k=100, top_p=0.7)\n",
    "                    # probs = F.softmax(logits, -1)\n",
    "                    # prev_input = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "                    # add to generated tokens list\n",
    "                    count = 0\n",
    "                    for idx, value in enumerate(flags):\n",
    "                        if value != 0:\n",
    "                            generated_tokens[idx].append(np_prev_tokens[count])\n",
    "                            count += 1\n",
    "\n",
    "                    # filtering algorithm\n",
    "                    if np.any(np_prev_tokens == 628):\n",
    "                        # set flags 0\n",
    "                        count = 0\n",
    "                        for idx, value in enumerate(flags):\n",
    "                            if value == 1:\n",
    "                                if np_prev_tokens[count] == 628:\n",
    "                                    flags[idx] = 0\n",
    "                                count += 1\n",
    "                        # compute which one to keep\n",
    "                        keep_indices = np.argwhere(np_prev_tokens != 628).squeeze(1)\n",
    "                        # filter\n",
    "                        prev_tokens = prev_tokens[keep_indices.tolist()]\n",
    "                        cur_pos = cur_pos[keep_indices.tolist(), :]\n",
    "                        temp_mask = temp_mask[keep_indices.tolist(), :]\n",
    "                        temp_past = [item[:, keep_indices.tolist()] for item in temp_past]\n",
    "                        np_prev_tokens = np_prev_tokens[keep_indices.tolist()]\n",
    "\n",
    "                    if np.all(flags == 0):\n",
    "                        break\n",
    "\n",
    "                    # prepare for the next token        \n",
    "                    temp_mask = F.pad(temp_mask, pad=(0, 1), value=1)\n",
    "                    logits, temp_past = model_B(prev_tokens.view(-1, 1), \n",
    "                                           position_ids=cur_pos, \n",
    "                                           mask=temp_mask, \n",
    "                                           past=temp_past)\n",
    "                    cur_pos = cur_pos + 1\n",
    "\n",
    "                # real system_tokens feed in\n",
    "                mask = torch.cat([mask, system_mask], dim=-1)\n",
    "                _, past = model_B(system_tokens, position_ids=system_pos, mask=mask, past=past)\n",
    "\n",
    "                # inject into generated_responses_list\n",
    "                decoded_responses = [tokenizer.decode(item).replace(\"\\n\", \"\") for item in generated_tokens]\n",
    "                count = 0\n",
    "                for idx in batch_keep_indices[turn_num]:\n",
    "                    generated_responses[idx].append(decoded_responses[count])\n",
    "                    count += 1\n",
    "\n",
    "            # add to the final responses        \n",
    "            for item in generated_responses:\n",
    "                all_response.extend(item)\n",
    "                \n",
    "    # Stage 2\n",
    "    #   prepare for metric eval\n",
    "    dialog_data = []\n",
    "    count = 0\n",
    "    all_results = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        raw_dialog = data[i]\n",
    "\n",
    "        for turn_num in range(len(raw_dialog)):\n",
    "\n",
    "            replaced_response = clean_sentence(\n",
    "                replace_punc(raw_dialog[turn_num][\"replaced_response\"].lower().replace(\"slot\", \"SLOT\")), entity_dict)\n",
    "\n",
    "            generated_response = clean_sentence(replace_punc(all_response[count].lower().replace(\"slot\", \"SLOT\")), entity_dict)\n",
    "\n",
    "            dialog_data.append({\"dial_id\": raw_dialog[turn_num][\"dial_id\"],\n",
    "                                \"turn_num\": raw_dialog[turn_num][\"turn_num\"],\n",
    "                                \"response\": replaced_response,\n",
    "                                \"generated_response\":generated_response \n",
    "                              })\n",
    "            count += 1\n",
    "            \n",
    "    sccuess_f1 = success_f1_metric(dialog_data)\n",
    "    bleu = bleu_metric(dialog_data)\n",
    "\n",
    "    return {\"bleu\": bleu,\n",
    "            \"sccuess_f1\": sccuess_f1\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinggu/anaconda3/envs/pretraining_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe0911878104dce94ed9523f2da91d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e7a5d9a54cfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_dialogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_keep_indices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mrecord_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dialogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_keep_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mupdate_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-69cf34ca6d24>\u001b[0m in \u001b[0;36mtrain_one_iter\u001b[0;34m(batch_dialogs, batch_keep_indices, update_count, fp16)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# User Utterance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mall_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'mask'"
     ]
    }
   ],
   "source": [
    "update_count = 0\n",
    "progress_bar = tqdm.tqdm_notebook\n",
    "start = time.time()\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "\n",
    "    \"Training\"\n",
    "    pbar = progress_bar(train_dataloader)\n",
    "    model_A.train()\n",
    "    model_B.train()\n",
    "    \n",
    "    for batch_dialogs, batch_keep_indices in pbar:\n",
    "        \n",
    "        record_loss, perplexity = train_one_iter(batch_dialogs, batch_keep_indices, update_count, fp16=False)\n",
    "\n",
    "        update_count += 1\n",
    "\n",
    "        if update_count % num_gradients_accumulation == num_gradients_accumulation - 1:\n",
    "            # update for gradient accumulation\n",
    "            \n",
    "#             torch.nn.utils.clip_grad_norm_(model_A.parameters(), 5.0)\n",
    "#             torch.nn.utils.clip_grad_norm_(model_B.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # speed measure\n",
    "            end = time.time()\n",
    "            speed = train_batch_size * num_gradients_accumulation / (end - start)\n",
    "            start = end\n",
    "            \n",
    "            # show progress\n",
    "            pbar.set_postfix(loss=record_loss, perplexity=perplexity, speed=speed)\n",
    "    \n",
    "    \"Evaluation\"\n",
    "    print(f\"Epoch {ep} Validation\")\n",
    "    eval_res = validate(val_dataloader, val_data)\n",
    "    print(eval_res)\n",
    "    \n",
    "    print(f\"Epoch {ep} Test\")\n",
    "    eval_res = validate(test_dataloader, test_data)\n",
    "    print(eval_res)\n",
    "    \n",
    "    checkpointer.save_checkpoint(ep, \n",
    "                                 [model_A.state_dict(), model_A.state_dict()],\n",
    "                                 {\"None\": None},\n",
    "                                 True\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c5bb41b1074b75b937ecf0c536ed37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "This tokenizer does not make use of special tokens.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0ab1ddbccef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_dialogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_keep_indices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mrecord_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dialogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_keep_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mupdate_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-69cf34ca6d24>\u001b[0m in \u001b[0;36mtrain_one_iter\u001b[0;34m(batch_dialogs, batch_keep_indices, update_count, fp16)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# User Utterance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mall_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'mask'"
     ]
    }
   ],
   "source": [
    "update_count = 0\n",
    "progress_bar = tqdm.notebook.tqdm\n",
    "start = time.time()\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "\n",
    "    \"Training\"\n",
    "    pbar = progress_bar(train_dataloader)\n",
    "    model_A.train()\n",
    "    model_B.train()\n",
    "    \n",
    "    for batch_dialogs, batch_keep_indices in pbar:\n",
    "        \n",
    "        record_loss, perplexity = train_one_iter(batch_dialogs, batch_keep_indices, update_count, fp16=False)\n",
    "\n",
    "        update_count += 1\n",
    "\n",
    "        if update_count % num_gradients_accumulation == num_gradients_accumulation - 1:\n",
    "            # update for gradient accumulation\n",
    "            \n",
    "#             torch.nn.utils.clip_grad_norm_(model_A.parameters(), 5.0)\n",
    "#             torch.nn.utils.clip_grad_norm_(model_B.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # speed measure\n",
    "            end = time.time()\n",
    "            speed = train_batch_size * num_gradients_accumulation / (end - start)\n",
    "            start = end\n",
    "            \n",
    "            # show progress\n",
    "            pbar.set_postfix(loss=record_loss, perplexity=perplexity, speed=speed)\n",
    "    \n",
    "    \"Evaluation\"\n",
    "    print(f\"Epoch {ep} Validation\")\n",
    "    eval_res = validate(val_dataloader, val_data)\n",
    "    print(eval_res)\n",
    "    \n",
    "    print(f\"Epoch {ep} Test\")\n",
    "    eval_res = validate(test_dataloader, test_data)\n",
    "    print(eval_res)\n",
    "    \n",
    "    checkpointer.save_checkpoint(ep, \n",
    "                                 [model_A.state_dict(), model_A.state_dict()],\n",
    "                                 {\"None\": None},\n",
    "                                 True\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'bleu': 0.18528576144485603, 'sccuess_f1': 0.7661691492358308}\n",
    "Epoch 0 Test\n",
    "100%\n",
    "9/9 [00:57<00:00, 6.43s/it]\n",
    "\n",
    "{'bleu': 0.18891712428744042, 'sccuess_f1': 0.7538619929578371}\n",
    "100%\n",
    "200/200 [43:15<00:00, 12.98s/it, loss=0.263, perplexity=1.3, speed=2.11]\n",
    "\n",
    "Epoch 1 Validation\n",
    "100%\n",
    "9/9 [01:19<00:00, 8.81s/it]\n",
    "\n",
    "{'bleu': 0.15647152396609024, 'sccuess_f1': 0.844407059794514}\n",
    "Epoch 1 Test\n",
    "100%\n",
    "9/9 [00:48<00:00, 5.38s/it]\n",
    "\n",
    "{'bleu': 0.15899872884357819, 'sccuess_f1': 0.828924157270311}\n",
    "100%\n",
    "200/200 [41:01<00:00, 12.31s/it, loss=0.286, perplexity=1.33, speed=2.18]\n",
    "\n",
    "Epoch 2 Validation\n",
    "100%\n",
    "9/9 [00:39<00:00, 4.41s/it]\n",
    "\n",
    "{'bleu': 0.18037047407794105, 'sccuess_f1': 0.7925998002521975}\n",
    "Epoch 2 Test\n",
    "100%\n",
    "9/9 [00:17<00:00, 2.00s/it]\n",
    "\n",
    "{'bleu': 0.1848661149382996, 'sccuess_f1': 0.7939999949966201}\n",
    "100%\n",
    "200/200 [39:07<00:00, 11.74s/it, loss=0.272, perplexity=1.31, speed=3.53]\n",
    "\n",
    "Epoch 3 Validation\n",
    "100%\n",
    "9/9 [00:17<00:00, 1.98s/it]\n",
    "\n",
    "{'bleu': 0.13512004199985797, 'sccuess_f1': 0.7270765862609081}\n",
    "Epoch 3 Test\n",
    "100%\n",
    "9/9 [01:03<00:00, 7.10s/it]\n",
    "\n",
    "{'bleu': 0.13928672975886824, 'sccuess_f1': 0.717149215617259}\n",
    "100%\n",
    "200/200 [37:20<00:00, 11.20s/it, loss=0.209, perplexity=1.23, speed=2.71]\n",
    "\n",
    "Epoch 4 Validation\n",
    "100%\n",
    "9/9 [00:55<00:00, 6.20s/it]\n",
    "\n",
    "{'bleu': 0.20840774459932415, 'sccuess_f1': 0.8462222172182756}\n",
    "Epoch 4 Test\n",
    "100%\n",
    "9/9 [00:30<00:00, 3.42s/it]\n",
    "\n",
    "{'bleu': 0.21322651574953702, 'sccuess_f1': 0.8427787884113279}\n",
    "100%\n",
    "200/200 [35:17<00:00, 10.59s/it, loss=0.217, perplexity=1.24, speed=2.35]\n",
    "\n",
    "Epoch 5 Validation\n",
    "100%\n",
    "9/9 [00:49<00:00, 5.48s/it]\n",
    "\n",
    "{'bleu': 0.19571594985463295, 'sccuess_f1': 0.8317580290116173}\n",
    "Epoch 5 Test\n",
    "100%\n",
    "9/9 [00:23<00:00, 2.61s/it]\n",
    "\n",
    "{'bleu': 0.2072375771999586, 'sccuess_f1': 0.8360814692822786}\n",
    "100%\n",
    "200/200 [33:13<00:00, 9.97s/it, loss=0.187, perplexity=1.21, speed=2.13]\n",
    "\n",
    "Epoch 6 Validation\n",
    "100%\n",
    "9/9 [00:25<00:00, 2.83s/it]\n",
    "\n",
    "{'bleu': 0.20805844245958913, 'sccuess_f1': 0.8542056024606866}\n",
    "Epoch 6 Test\n",
    "100%\n",
    "9/9 [00:24<00:00, 2.69s/it]\n",
    "\n",
    "{'bleu': 0.2053606768192143, 'sccuess_f1': 0.8547008496846618}\n",
    "100%\n",
    "200/200 [31:07<00:00, 9.34s/it, loss=0.294, perplexity=1.34, speed=3.93]\n",
    "\n",
    "Epoch 7 Validation\n",
    "100%\n",
    "9/9 [01:06<00:00, 7.36s/it]\n",
    "\n",
    "{'bleu': 0.20471616971302903, 'sccuess_f1': 0.8595348787049779}\n",
    "Epoch 7 Test\n",
    "100%\n",
    "9/9 [00:42<00:00, 4.71s/it]\n",
    "\n",
    "{'bleu': 0.20968254546296883, 'sccuess_f1': 0.857414443652919}\n",
    "100%\n",
    "200/200 [29:02<00:00, 8.71s/it, loss=0.143, perplexity=1.15, speed=1.71]\n",
    "\n",
    "Epoch 8 Validation\n",
    "100%\n",
    "9/9 [00:57<00:00, 6.43s/it]\n",
    "\n",
    "{'bleu': 0.20096424029373133, 'sccuess_f1': 0.839622636494233}\n",
    "Epoch 8 Test\n",
    "100%\n",
    "9/9 [00:32<00:00, 3.56s/it]\n",
    "\n",
    "{'bleu': 0.19602954287333282, 'sccuess_f1': 0.8489483697448351}\n",
    "100%\n",
    "200/200 [26:55<00:00, 8.08s/it, loss=0.199, perplexity=1.22, speed=3.24]\n",
    "\n",
    "Epoch 9 Validation\n",
    "100%\n",
    "9/9 [00:49<00:00, 5.51s/it]\n",
    "\n",
    "{'bleu': 0.19466862379128747, 'sccuess_f1': 0.8205128154988641}\n",
    "Epoch 9 Test\n",
    "100%\n",
    "9/9 [00:24<00:00, 2.67s/it]\n",
    "\n",
    "{'bleu': 0.1984431006396542, 'sccuess_f1': 0.8428158098350613}\n",
    "1\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pretraining_env]",
   "language": "python",
   "name": "conda-env-pretraining_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
