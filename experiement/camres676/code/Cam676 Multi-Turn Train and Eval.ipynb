{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchfly\n",
    "torchfly.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import random\n",
    "import itertools\n",
    "import tqdm\n",
    "import time\n",
    "from torchfly.utils.model_utils import get_pretrained_states\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except:\n",
    "    from tensorboardX import SummaryWriter\n",
    "from apex import amp\n",
    "from allennlp.training.checkpointer import Checkpointer\n",
    "# from pytorch_transformers import AdamW, WarmupLinearSchedule, GPT2Tokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import WarmupLinearSchedule\n",
    "from torchfly.text.tokenizers import UnifiedBPETokenizer\n",
    "\n",
    "from torchfly.modules.losses import SequenceFocalLoss, SequenceCrossEntropyLoss\n",
    "from torchfly.modules.transformers import GPT2SimpleLM, UnifiedGPT2SmallConfig\n",
    "from cam676_eval.cam676_eval import clean_sentence, entities, entity_dict, success_f1_metric, bleu_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set tokenizer\n",
    "tokenizer = UnifiedBPETokenizer()\n",
    "tokenizer.sep_token = \"None\"\n",
    "# add speicial tokens in the same order as Roberta\n",
    "# tokenizer.add_tokens([\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass GPT2SmallConfig:\\n    vocab_size = 50257 + len(tokenizer.added_tokens_encoder)\\n    n_special = len(tokenizer.added_tokens_encoder)\\n    n_positions = 1024\\n    n_ctx = 1024\\n    n_embd = 768\\n    n_layer = 12\\n    n_head = 12\\n    resid_pdrop = 0.1\\n    embd_pdrop = 0.1\\n    attn_pdrop = 0.1\\n    layer_norm_epsilon = 1e-5\\n    initializer_range = 0.02\\n    gradient_checkpointing = False\\n    \\nclass GPT2MediumConfig:\\n    vocab_size = len(tokenizer.added_tokens_encoder)\\n    n_special = len(tokenizer.added_tokens_encoder)\\n    n_positions = 1024\\n    n_ctx = 1024\\n    n_embd = 1024\\n    n_layer = 24\\n    n_head = 16\\n    resid_pdrop = 0.1\\n    embd_pdrop = 0.1\\n    attn_pdrop = 0.1\\n    layer_norm_epsilon = 1e-5\\n    initializer_range = 0.02\\n    gradient_checkpointing = True\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class GPT2SmallConfig:\n",
    "    vocab_size = 50257 + len(tokenizer.added_tokens_encoder)\n",
    "    n_special = len(tokenizer.added_tokens_encoder)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 768\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = False\n",
    "    \n",
    "class GPT2MediumConfig:\n",
    "    vocab_size = len(tokenizer.added_tokens_encoder)\n",
    "    n_special = len(tokenizer.added_tokens_encoder)\n",
    "    n_positions = 1024\n",
    "    n_ctx = 1024\n",
    "    n_embd = 1024\n",
    "    n_layer = 24\n",
    "    n_head = 16\n",
    "    resid_pdrop = 0.1\n",
    "    embd_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "    layer_norm_epsilon = 1e-5\n",
    "    initializer_range = 0.02\n",
    "    gradient_checkpointing = True\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: /home/jinggu/.cache/torchfly/models/unified-gpt2-small.pth\n",
      "File exists: /home/jinggu/.cache/torchfly/models/unified-gpt2-small.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A = GPT2SimpleLM(UnifiedGPT2SmallConfig)\n",
    "model_B = GPT2SimpleLM(UnifiedGPT2SmallConfig)\n",
    "# model_A.load_state_dict(torch.load(\"../../../Checkpoint/best.th\"))\n",
    "# model_B.load_state_dict(torch.load(\"../../../Checkpoint/best.th\"))\n",
    "model_A.load_state_dict(get_pretrained_states(\"unified-gpt2-small\"))\n",
    "model_B.load_state_dict(get_pretrained_states(\"unified-gpt2-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_keep_indices(batch_keep_indices):\n",
    "    prev = batch_keep_indices[1]\n",
    "    new_batch_keep_indices = [prev]\n",
    "\n",
    "    for i in range(1, len(batch_keep_indices)):\n",
    "        curr = batch_keep_indices[i]\n",
    "        new = []\n",
    "\n",
    "        for idx in curr:\n",
    "            new.append(prev.index(idx))\n",
    "\n",
    "        new_batch_keep_indices.append(new)\n",
    "        prev = curr\n",
    "        \n",
    "    return new_batch_keep_indices\n",
    "\n",
    "\n",
    "class CamRestDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bos = tokenizer.encode(\"<s>\")\n",
    "        self.user_bos = tokenizer.encode(\"A:\")\n",
    "        self.system_bos = tokenizer.encode(\"B:\")\n",
    "        \n",
    "        self.eos = [628, 198]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        full_dialog = self.data[index]\n",
    "        \n",
    "        full_dialog_tokens = []\n",
    "        cur_pos = 0\n",
    "        \n",
    "        for turn_dialog in full_dialog:\n",
    "            # user\n",
    "            user_tokens = self.user_bos + tokenizer.encode(turn_dialog['user']) + self.eos\n",
    "            user_pos = torch.arange(cur_pos, cur_pos + len(user_tokens))\n",
    "            cur_pos = user_pos[-1] + 1\n",
    "            \n",
    "            # belief span\n",
    "            belief_tokens = self.bos + \\\n",
    "                            tokenizer.encode(\";\".join(turn_dialog['bspan_inform'][1:])) + \\\n",
    "                            self.eos\n",
    "            belief_pos = torch.arange(cur_pos, cur_pos + len(belief_tokens))\n",
    "            cur_pos = belief_pos[-1]\n",
    "            \n",
    "            # system\n",
    "            if np.random.rand() < 0.04:\n",
    "                turn_dialog[\"degree\"] = 0 \n",
    "            database = tokenizer.encode(str(turn_dialog[\"degree\"]))\n",
    "            # database_pos = torch.LongTensor([1023])\n",
    "            \n",
    "            system_tokens = self.system_bos + \\\n",
    "                            tokenizer.encode(turn_dialog['replaced_response']) + \\\n",
    "                            self.eos\n",
    "            system_pos = torch.arange(cur_pos, cur_pos + len(system_tokens) + 1)\n",
    "            cur_pos = system_pos[-1] + 1\n",
    "            \n",
    "            # concat database and response\n",
    "            system_tokens = database + system_tokens\n",
    "            # system_pos = torch.cat([database_pos, system_pos], dim=0)\n",
    "            \n",
    "            user_tokens = torch.LongTensor(user_tokens)\n",
    "            system_tokens = torch.LongTensor(system_tokens)\n",
    "            belief_tokens = torch.LongTensor(belief_tokens)\n",
    "            \n",
    "            full_dialog_tokens.append((user_tokens, \n",
    "                                       user_pos, \n",
    "                                       system_tokens, \n",
    "                                       system_pos, \n",
    "                                       belief_tokens, \n",
    "                                       belief_pos))\n",
    "\n",
    "        return full_dialog_tokens\n",
    "        \n",
    "\n",
    "class Collate_Function:\n",
    "    \"\"\"This function handles batch collate.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad = self.tokenizer.encode(\"<pad>\")[0]\n",
    "\n",
    "    def __call__(self, unpacked_data):\n",
    "\n",
    "        max_turn_len = max([len(item) for item in unpacked_data])\n",
    "        \n",
    "        batch_dialogs = []\n",
    "        batch_keep_indices = []\n",
    "\n",
    "        for turn_num in range(max_turn_len):\n",
    "\n",
    "            keep_indices = []\n",
    "\n",
    "            for batch_idx in range(len(unpacked_data)):\n",
    "                if turn_num < len(unpacked_data[batch_idx]):\n",
    "                    keep_indices.append(batch_idx)\n",
    "\n",
    "            user_tokens = pad_sequence([unpacked_data[idx][turn_num][0] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            user_pos = pad_sequence([unpacked_data[idx][turn_num][1] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)\n",
    "            system_tokens = pad_sequence([unpacked_data[idx][turn_num][2] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            system_pos = pad_sequence([unpacked_data[idx][turn_num][3] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)\n",
    "            belief_tokens = pad_sequence([unpacked_data[idx][turn_num][4] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=self.pad)\n",
    "            belief_pos = pad_sequence([unpacked_data[idx][turn_num][5] for idx in keep_indices], \n",
    "                                        batch_first=True,\n",
    "                                        padding_value=0)  \n",
    "\n",
    "            user_mask = (user_tokens != self.pad).byte()\n",
    "            system_mask = (system_tokens != self.pad).byte()\n",
    "            belief_mask = (belief_tokens != self.pad).byte()\n",
    "\n",
    "\n",
    "            batch_dialogs.append((user_tokens, user_pos, user_mask, \n",
    "                                  system_tokens, system_pos, system_mask, \n",
    "                                  belief_tokens, belief_pos, belief_mask))\n",
    "            batch_keep_indices.append(keep_indices)\n",
    "            \n",
    "        # align keep indices\n",
    "        # batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "        return batch_dialogs, batch_keep_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(logits, target, mask):\n",
    "    logits = logits[:, :-1].contiguous()\n",
    "    target = target[:, 1:].contiguous()\n",
    "    mask = mask[:, 1:].contiguous().float()\n",
    "    loss = criterion(logits, target, mask, label_smoothing=0.02, reduce=True)\n",
    "    return loss\n",
    "\n",
    "def filter_past(past, keep_indices):\n",
    "    past = [item[:, keep_indices] for item in past]\n",
    "    return past\n",
    "\n",
    "def replace_punc(x):\n",
    "    x = x.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "    return x.replace(\".\", \" .\").replace(\",\", \" .\").replace(\"?\", \" ?\").replace(\"?\", \" ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load(\"../data/DataProcess/train_data.pkl\")\n",
    "val_data = torch.load(\"../data/DataProcess/val_data.pkl\")\n",
    "test_data = torch.load(\"../data/DataProcess/test_data.pkl\")\n",
    "\n",
    "indices = np.arange(len(train_data))\n",
    "np.random.shuffle(indices)\n",
    "# use all data\n",
    "indices = indices[: 200]\n",
    "train_data = [train_data[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CamRestDataset(train_data, tokenizer)\n",
    "val_dataset = CamRestDataset(val_data, tokenizer)\n",
    "test_dataset = CamRestDataset(test_data, tokenizer)\n",
    "\n",
    "train_batch_size = 1\n",
    "collate_func = Collate_Function(tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              shuffle=True,\n",
    "                              batch_size=train_batch_size, \n",
    "                              collate_fn=collate_func)\n",
    "\n",
    "eval_batch_size = 16\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_dataset, \n",
    "                          shuffle=False,\n",
    "                          batch_size=eval_batch_size, \n",
    "                          collate_fn=collate_func)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_dataset, \n",
    "                          shuffle=False,\n",
    "                          batch_size=eval_batch_size, \n",
    "                          collate_fn=collate_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SequenceFocalLoss(gamma=0.0, beta=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_A = model_A.to(device)\n",
    "model_B = model_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"Checkpoint\"):\n",
    "    os.makedirs(\"Checkpoint\")\n",
    "checkpointer = Checkpointer(serialization_dir=\"Checkpoint\", \n",
    "                            keep_serialized_model_every_num_seconds=3600*2, \n",
    "                            num_serialized_models_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "num_epochs = 10\n",
    "num_gradients_accumulation = 1\n",
    "num_train_optimization_steps = num_train_optimization_steps = len(train_dataset) * num_epochs // train_batch_size // num_gradients_accumulation\n",
    "\n",
    "param_optimizer = list(model_A.named_parameters()) + list(model_B.named_parameters())\n",
    "no_decay = ['ln', 'bias', 'LayerNorm']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                  lr=5e-5,\n",
    "                  correct_bias=False)\n",
    "\n",
    "scheduler = WarmupLinearSchedule(optimizer,\n",
    "                                 warmup_steps=500,\n",
    "                                 t_total=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [model_A, model_B], optimizer = amp.initialize([model_A, model_B], optimizer, opt_level=\"O0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_weight = 1.0\n",
    "\n",
    "def train_one_iter(batch_dialogs, batch_keep_indices, update_count, fp16=False):\n",
    "\n",
    "    aligned_batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "   \n",
    "    mask = torch.ByteTensor([]).to(device)\n",
    "    prev_batch_size = batch_dialogs[0][0].shape[0]\n",
    "    \n",
    "\n",
    "    past = None\n",
    "    all_logits = []\n",
    "    target = []\n",
    "    total_loss = 0 \n",
    "    \n",
    "    for turn_num in range(len(batch_keep_indices)):\n",
    "\n",
    "        # data send to gpu\n",
    "        dialogs = batch_dialogs[turn_num]\n",
    "        dialogs = [item.to(device) for item in dialogs]\n",
    "\n",
    "        user_tokens, user_pos, user_mask, \\\n",
    "            system_tokens, system_pos, system_mask, \\\n",
    "            belief_tokens, belief_pos, belief_mask = dialogs\n",
    "\n",
    "        # filtering algorithm\n",
    "        keep_indices = aligned_batch_keep_indices[turn_num]\n",
    "\n",
    "        if len(keep_indices) != prev_batch_size:\n",
    "            past = filter_past(past, keep_indices)\n",
    "            mask = mask[keep_indices, :]\n",
    "\n",
    "        # User Utterance\n",
    "        mask = torch.cat([mask, user_mask], dim=-1)\n",
    "        logits, past = model_A(user_tokens, position_ids=user_pos, mask=mask, past=past)\n",
    "        all_logits.append(logits)\n",
    "        target.append(user_tokens)\n",
    "        # A_loss = calculate_loss(logits, user_tokens, user_mask)\n",
    "\n",
    "        # System Response\n",
    "        mask = torch.cat([mask, system_mask], dim=-1)\n",
    "        logits, past = model_B(system_tokens, position_ids=system_pos, mask=mask, past=past)\n",
    "        all_logits.append(logits)\n",
    "        target.append(system_tokens)\n",
    "        # B_loss = calculate_loss(logits, system_tokens, system_mask)\n",
    "\n",
    "        # tail\n",
    "        # total_loss = total_loss + user_weight * A_loss + B_loss\n",
    "        prev_batch_size = user_tokens.shape[0]\n",
    "\n",
    "#     breakpoint\n",
    "    all_logits = torch.cat(all_logits, dim=1)\n",
    "    all_logits = all_logits[:, :-1].contiguous()\n",
    "\n",
    "    target = torch.cat(target, dim=1)\n",
    "    target = target[:, 1:].contiguous()\n",
    "    \n",
    "    target_mask = torch.ones_like(target).float()\n",
    "    \n",
    "    total_loss = criterion(all_logits, target, target_mask, label_smoothing=0.02, reduce=True)\n",
    "\n",
    "    # gradient accumulation\n",
    "    total_loss /= len(batch_keep_indices)\n",
    "    total_loss /= num_gradients_accumulation \n",
    "    \n",
    "    if fp16:\n",
    "        with amp.scale_loss(total_loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "    else:\n",
    "        total_loss.backward()\n",
    "        \n",
    "    record_loss = total_loss.item() * num_gradients_accumulation\n",
    "    perplexity = np.exp(record_loss)\n",
    "    \n",
    "    return record_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, data):\n",
    "\n",
    "    model_A.eval()\n",
    "    model_B.eval()\n",
    "\n",
    "    temperature = 0.5\n",
    "\n",
    "    all_response = []\n",
    "\n",
    "    for batch_dialogs, batch_keep_indices in tqdm.notebook.tqdm(dataloader):\n",
    "\n",
    "        aligned_batch_keep_indices = align_keep_indices(batch_keep_indices)\n",
    "        past = None\n",
    "        generated_responses = [[] for i in range(batch_dialogs[0][0].shape[0])]\n",
    "\n",
    "        mask = torch.ByteTensor([]).to(device)\n",
    "        prev_batch_size = batch_dialogs[0][0].shape[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for turn_num in range(len(batch_keep_indices)):\n",
    "                # data send to gpu\n",
    "                dialogs = batch_dialogs[turn_num]\n",
    "                dialogs = [item.to(device) for item in dialogs]\n",
    "\n",
    "                user_tokens, user_pos, user_mask, \\\n",
    "                    system_tokens, system_pos, system_mask, \\\n",
    "                    belief_tokens, belief_pos, belief_mask = dialogs\n",
    "\n",
    "                # batch filtering algorithm\n",
    "                keep_indices = aligned_batch_keep_indices[turn_num]\n",
    "\n",
    "                if len(keep_indices) != prev_batch_size:\n",
    "                    past = filter_past(past, keep_indices)\n",
    "                    mask = mask[keep_indices, :]\n",
    "\n",
    "                # define some initials\n",
    "                cur_batch_size = user_tokens.shape[0]\n",
    "                flags = np.ones(cur_batch_size)\n",
    "                generated_tokens = [[] for i in range(cur_batch_size)]\n",
    "\n",
    "                # feed in user\n",
    "                mask = torch.cat([mask, user_mask], dim=-1)\n",
    "                _, past = model_A(user_tokens, position_ids=user_pos, mask=mask, past=past)\n",
    "\n",
    "                # response generation\n",
    "                response = []\n",
    "\n",
    "\n",
    "                # first three tokens\n",
    "                prev_input = system_tokens[:, :3]\n",
    "                cur_pos = system_pos[:, :3]\n",
    "                temp_past = past\n",
    "                temp_mask = F.pad(mask, pad=(0,3), value=1)\n",
    "\n",
    "                # feed into B\n",
    "                logits, temp_past = model_B(prev_input, position_ids=cur_pos, mask=temp_mask, past=temp_past)\n",
    "                # set current position\n",
    "                cur_pos = cur_pos[:, -1].unsqueeze(1) + 1\n",
    "\n",
    "                for i in range(50):\n",
    "                    logits = logits[:, -1, :] / temperature\n",
    "                    prev_tokens = torch.argmax(logits, dim=-1)\n",
    "                    np_prev_tokens = prev_tokens.cpu().numpy()\n",
    "                    # nucleus sampling\n",
    "                    # logits = top_filtering(logits, top_k=100, top_p=0.7)\n",
    "                    # probs = F.softmax(logits, -1)\n",
    "                    # prev_input = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "                    # add to generated tokens list\n",
    "                    count = 0\n",
    "                    for idx, value in enumerate(flags):\n",
    "                        if value != 0:\n",
    "                            generated_tokens[idx].append(np_prev_tokens[count])\n",
    "                            count += 1\n",
    "\n",
    "                    # filtering algorithm\n",
    "                    if np.any(np_prev_tokens == 628):\n",
    "                        # set flags 0\n",
    "                        count = 0\n",
    "                        for idx, value in enumerate(flags):\n",
    "                            if value == 1:\n",
    "                                if np_prev_tokens[count] == 628:\n",
    "                                    flags[idx] = 0\n",
    "                                count += 1\n",
    "                        # compute which one to keep\n",
    "                        keep_indices = np.argwhere(np_prev_tokens != 628).squeeze(1)\n",
    "                        # filter\n",
    "                        prev_tokens = prev_tokens[keep_indices.tolist()]\n",
    "                        cur_pos = cur_pos[keep_indices.tolist(), :]\n",
    "                        temp_mask = temp_mask[keep_indices.tolist(), :]\n",
    "                        temp_past = [item[:, keep_indices.tolist()] for item in temp_past]\n",
    "                        np_prev_tokens = np_prev_tokens[keep_indices.tolist()]\n",
    "\n",
    "                    if np.all(flags == 0):\n",
    "                        break\n",
    "\n",
    "                    # prepare for the next token        \n",
    "                    temp_mask = F.pad(temp_mask, pad=(0, 1), value=1)\n",
    "                    logits, temp_past = model_B(prev_tokens.view(-1, 1), \n",
    "                                           position_ids=cur_pos, \n",
    "                                           mask=temp_mask, \n",
    "                                           past=temp_past)\n",
    "                    cur_pos = cur_pos + 1\n",
    "\n",
    "                # real system_tokens feed in\n",
    "                mask = torch.cat([mask, system_mask], dim=-1)\n",
    "                _, past = model_B(system_tokens, position_ids=system_pos, mask=mask, past=past)\n",
    "\n",
    "                # inject into generated_responses_list\n",
    "                decoded_responses = [tokenizer.decode(item).replace(\"\\n\", \"\") for item in generated_tokens]\n",
    "                count = 0\n",
    "                for idx in batch_keep_indices[turn_num]:\n",
    "                    generated_responses[idx].append(decoded_responses[count])\n",
    "                    count += 1\n",
    "\n",
    "            # add to the final responses        \n",
    "            for item in generated_responses:\n",
    "                all_response.extend(item)\n",
    "                \n",
    "    # Stage 2\n",
    "    #   prepare for metric eval\n",
    "    dialog_data = []\n",
    "    count = 0\n",
    "    all_results = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        raw_dialog = data[i]\n",
    "\n",
    "        for turn_num in range(len(raw_dialog)):\n",
    "\n",
    "            replaced_response = clean_sentence(\n",
    "                replace_punc(raw_dialog[turn_num][\"replaced_response\"].lower().replace(\"slot\", \"SLOT\")), entity_dict)\n",
    "\n",
    "            generated_response = clean_sentence(replace_punc(all_response[count].lower().replace(\"slot\", \"SLOT\")), entity_dict)\n",
    "\n",
    "            dialog_data.append({\"dial_id\": raw_dialog[turn_num][\"dial_id\"],\n",
    "                                \"turn_num\": raw_dialog[turn_num][\"turn_num\"],\n",
    "                                \"response\": replaced_response,\n",
    "                                \"generated_response\":generated_response \n",
    "                              })\n",
    "            count += 1\n",
    "            \n",
    "    sccuess_f1 = success_f1_metric(dialog_data)\n",
    "    bleu = bleu_metric(dialog_data)\n",
    "\n",
    "    return {\"bleu\": bleu,\n",
    "            \"sccuess_f1\": sccuess_f1\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinggu/anaconda3/envs/pretraining_env/lib/python3.7/site-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26079b0e8232437ca316d2cb2be319fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinggu/.local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b444f3c6ef34b3b853da5c77d6eb8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.17883829935113943, 'sccuess_f1': 0.7996219231521114}\n",
      "Epoch 0 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232a3e2cfcba4d70a869d3b3550f3fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.18003804574050458, 'sccuess_f1': 0.7992202678916399}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688f05ab1ccc49a6bb1aacd3330304a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ffaf76f4f44ecdaef88d97ddc7a6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.14628724800629037, 'sccuess_f1': 0.8348856851215943}\n",
      "Epoch 1 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266b0325474d45539aee3080f2095e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.15327392870292597, 'sccuess_f1': 0.8228070125605879}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b6f4b8737c4c35be5f3ff4bfb11d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9b8058e51d4cefb9edf0686947949e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.17569341358851026, 'sccuess_f1': 0.783889975352303}\n",
      "Epoch 2 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6666f97dda5b46559d66b139ddbbbe1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.186938638841664, 'sccuess_f1': 0.7984189673232867}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505bcdb8a0c44c3496bb6443b2d4a902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18ed407844042eba4ed536f90d45eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.12918408707058232, 'sccuess_f1': 0.680180175379535}\n",
      "Epoch 3 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0ce265cef74bfdb6bb4cacb273c565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.13361986771873902, 'sccuess_f1': 0.6925714237555984}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6583024ebc2415fb36ff24bc3b263e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f887ab650cd94a20b5ee70da779a72e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.20114153255239067, 'sccuess_f1': 0.8416289542652444}\n",
      "Epoch 4 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c46ad33d7454f96bbaf1ebef66b8bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.20880500448366363, 'sccuess_f1': 0.8359592164895069}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8838b4f9132747059dc97aa69fcef6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e27b5ab95eb4a099d14a25c36721fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.20453811015652118, 'sccuess_f1': 0.8341232177343006}\n",
      "Epoch 5 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9222e7d2a148e79de072c8344c1851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.21862632140785807, 'sccuess_f1': 0.8522072886499644}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84fbae130264ba5b24fbbc4fff6300f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4c7a4dff114a29809085a98438b73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.20199170192848068, 'sccuess_f1': 0.8542056024606866}\n",
      "Epoch 6 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d2104ef0254a749096cef6541da2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.19942324272622602, 'sccuess_f1': 0.8420038485489919}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7696a94a2fb49b0b0aa3a18da61376a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccb70e1096d4a6cb8ba9981dad37996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.20531761135564142, 'sccuess_f1': 0.841611991235506}\n",
      "Epoch 7 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fd94a41e584b58bfffef557f88541f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.21137626822789748, 'sccuess_f1': 0.8385826721544463}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fc2ad8e83c401d99f87f4297c7181c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc2fedba2734f969aef0bb903bcdb04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.20742733527745522, 'sccuess_f1': 0.8492136860114621}\n",
      "Epoch 8 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63a9b887f7743449dd62cc00fcc78a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.2030382818742682, 'sccuess_f1': 0.857414443652919}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6afa421a63b34c488a99885f31a39a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc14f1ad9ec84064a8f3ea2fa05a2429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.20296791173889112, 'sccuess_f1': 0.8485981258252774}\n",
      "Epoch 9 Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71e6a2d4d95445bab44cb53d383c218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'bleu': 0.2065926685437361, 'sccuess_f1': 0.8463034969313503}\n"
     ]
    }
   ],
   "source": [
    "update_count = 0\n",
    "progress_bar = tqdm.notebook.tqdm\n",
    "start = time.time()\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "\n",
    "    \"Training\"\n",
    "    pbar = progress_bar(train_dataloader)\n",
    "    model_A.train()\n",
    "    model_B.train()\n",
    "    \n",
    "    for batch_dialogs, batch_keep_indices in pbar:\n",
    "        \n",
    "        record_loss, perplexity = train_one_iter(batch_dialogs, batch_keep_indices, update_count, fp16=False)\n",
    "\n",
    "        update_count += 1\n",
    "\n",
    "        if update_count % num_gradients_accumulation == num_gradients_accumulation - 1:\n",
    "            # update for gradient accumulation\n",
    "            \n",
    "#             torch.nn.utils.clip_grad_norm_(model_A.parameters(), 5.0)\n",
    "#             torch.nn.utils.clip_grad_norm_(model_B.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # speed measure\n",
    "            end = time.time()\n",
    "            speed = train_batch_size * num_gradients_accumulation / (end - start)\n",
    "            start = end\n",
    "            \n",
    "            # show progress\n",
    "            pbar.set_postfix(loss=record_loss, perplexity=perplexity, speed=speed)\n",
    "    \n",
    "    \"Evaluation\"\n",
    "    print(f\"Epoch {ep} Validation\")\n",
    "    eval_res = validate(val_dataloader, val_data)\n",
    "    print(eval_res)\n",
    "    \n",
    "    print(f\"Epoch {ep} Test\")\n",
    "    eval_res = validate(test_dataloader, test_data)\n",
    "    print(eval_res)\n",
    "    \n",
    "    checkpointer.save_checkpoint(ep, \n",
    "                                 [model_A.state_dict(), model_A.state_dict()],\n",
    "                                 {\"None\": None},\n",
    "                                 True\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'bleu': 0.18528576144485603, 'sccuess_f1': 0.7661691492358308}\n",
    "Epoch 0 Test\n",
    "100%\n",
    "9/9 [00:57<00:00, 6.43s/it]\n",
    "\n",
    "{'bleu': 0.18891712428744042, 'sccuess_f1': 0.7538619929578371}\n",
    "100%\n",
    "200/200 [43:15<00:00, 12.98s/it, loss=0.263, perplexity=1.3, speed=2.11]\n",
    "\n",
    "Epoch 1 Validation\n",
    "100%\n",
    "9/9 [01:19<00:00, 8.81s/it]\n",
    "\n",
    "{'bleu': 0.15647152396609024, 'sccuess_f1': 0.844407059794514}\n",
    "Epoch 1 Test\n",
    "100%\n",
    "9/9 [00:48<00:00, 5.38s/it]\n",
    "\n",
    "{'bleu': 0.15899872884357819, 'sccuess_f1': 0.828924157270311}\n",
    "100%\n",
    "200/200 [41:01<00:00, 12.31s/it, loss=0.286, perplexity=1.33, speed=2.18]\n",
    "\n",
    "Epoch 2 Validation\n",
    "100%\n",
    "9/9 [00:39<00:00, 4.41s/it]\n",
    "\n",
    "{'bleu': 0.18037047407794105, 'sccuess_f1': 0.7925998002521975}\n",
    "Epoch 2 Test\n",
    "100%\n",
    "9/9 [00:17<00:00, 2.00s/it]\n",
    "\n",
    "{'bleu': 0.1848661149382996, 'sccuess_f1': 0.7939999949966201}\n",
    "100%\n",
    "200/200 [39:07<00:00, 11.74s/it, loss=0.272, perplexity=1.31, speed=3.53]\n",
    "\n",
    "Epoch 3 Validation\n",
    "100%\n",
    "9/9 [00:17<00:00, 1.98s/it]\n",
    "\n",
    "{'bleu': 0.13512004199985797, 'sccuess_f1': 0.7270765862609081}\n",
    "Epoch 3 Test\n",
    "100%\n",
    "9/9 [01:03<00:00, 7.10s/it]\n",
    "\n",
    "{'bleu': 0.13928672975886824, 'sccuess_f1': 0.717149215617259}\n",
    "100%\n",
    "200/200 [37:20<00:00, 11.20s/it, loss=0.209, perplexity=1.23, speed=2.71]\n",
    "\n",
    "Epoch 4 Validation\n",
    "100%\n",
    "9/9 [00:55<00:00, 6.20s/it]\n",
    "\n",
    "{'bleu': 0.20840774459932415, 'sccuess_f1': 0.8462222172182756}\n",
    "Epoch 4 Test\n",
    "100%\n",
    "9/9 [00:30<00:00, 3.42s/it]\n",
    "\n",
    "{'bleu': 0.21322651574953702, 'sccuess_f1': 0.8427787884113279}\n",
    "100%\n",
    "200/200 [35:17<00:00, 10.59s/it, loss=0.217, perplexity=1.24, speed=2.35]\n",
    "\n",
    "Epoch 5 Validation\n",
    "100%\n",
    "9/9 [00:49<00:00, 5.48s/it]\n",
    "\n",
    "{'bleu': 0.19571594985463295, 'sccuess_f1': 0.8317580290116173}\n",
    "Epoch 5 Test\n",
    "100%\n",
    "9/9 [00:23<00:00, 2.61s/it]\n",
    "\n",
    "{'bleu': 0.2072375771999586, 'sccuess_f1': 0.8360814692822786}\n",
    "100%\n",
    "200/200 [33:13<00:00, 9.97s/it, loss=0.187, perplexity=1.21, speed=2.13]\n",
    "\n",
    "Epoch 6 Validation\n",
    "100%\n",
    "9/9 [00:25<00:00, 2.83s/it]\n",
    "\n",
    "{'bleu': 0.20805844245958913, 'sccuess_f1': 0.8542056024606866}\n",
    "Epoch 6 Test\n",
    "100%\n",
    "9/9 [00:24<00:00, 2.69s/it]\n",
    "\n",
    "{'bleu': 0.2053606768192143, 'sccuess_f1': 0.8547008496846618}\n",
    "100%\n",
    "200/200 [31:07<00:00, 9.34s/it, loss=0.294, perplexity=1.34, speed=3.93]\n",
    "\n",
    "Epoch 7 Validation\n",
    "100%\n",
    "9/9 [01:06<00:00, 7.36s/it]\n",
    "\n",
    "{'bleu': 0.20471616971302903, 'sccuess_f1': 0.8595348787049779}\n",
    "Epoch 7 Test\n",
    "100%\n",
    "9/9 [00:42<00:00, 4.71s/it]\n",
    "\n",
    "{'bleu': 0.20968254546296883, 'sccuess_f1': 0.857414443652919}\n",
    "100%\n",
    "200/200 [29:02<00:00, 8.71s/it, loss=0.143, perplexity=1.15, speed=1.71]\n",
    "\n",
    "Epoch 8 Validation\n",
    "100%\n",
    "9/9 [00:57<00:00, 6.43s/it]\n",
    "\n",
    "{'bleu': 0.20096424029373133, 'sccuess_f1': 0.839622636494233}\n",
    "Epoch 8 Test\n",
    "100%\n",
    "9/9 [00:32<00:00, 3.56s/it]\n",
    "\n",
    "{'bleu': 0.19602954287333282, 'sccuess_f1': 0.8489483697448351}\n",
    "100%\n",
    "200/200 [26:55<00:00, 8.08s/it, loss=0.199, perplexity=1.22, speed=3.24]\n",
    "\n",
    "Epoch 9 Validation\n",
    "100%\n",
    "9/9 [00:49<00:00, 5.51s/it]\n",
    "\n",
    "{'bleu': 0.19466862379128747, 'sccuess_f1': 0.8205128154988641}\n",
    "Epoch 9 Test\n",
    "100%\n",
    "9/9 [00:24<00:00, 2.67s/it]\n",
    "\n",
    "{'bleu': 0.1984431006396542, 'sccuess_f1': 0.8428158098350613}\n",
    "1\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pretraining_env]",
   "language": "python",
   "name": "conda-env-pretraining_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
